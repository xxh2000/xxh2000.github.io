"use strict";(self.webpackChunkmy_website=self.webpackChunkmy_website||[]).push([[5611],{219:(e,n,a)=>{a.r(n),a.d(n,{assets:()=>l,contentTitle:()=>o,default:()=>p,frontMatter:()=>i,metadata:()=>s,toc:()=>c});var r=a(4848),t=a(8453);const i={custom_edit_url:null},o=void 0,s={id:"\u6570\u636e\u6e56/Iceberg/Iceberg\u5b9e\u8df5",title:"Iceberg\u5b9e\u8df5",description:"Iceberg\u96c6\u6210Hive",source:"@site/docs/\u6570\u636e\u6e56/Iceberg/Iceberg\u5b9e\u8df5.md",sourceDirName:"\u6570\u636e\u6e56/Iceberg",slug:"/\u6570\u636e\u6e56/Iceberg/Iceberg\u5b9e\u8df5",permalink:"/docs/\u6570\u636e\u6e56/Iceberg/Iceberg\u5b9e\u8df5",draft:!1,unlisted:!1,editUrl:null,tags:[],version:"current",frontMatter:{custom_edit_url:null},sidebar:"tutorialSidebar",previous:{title:"\u6570\u4ed3\u9762\u8bd5\u9898",permalink:"/docs/\u6570\u636e\u4ed3\u5e93/\u6570\u4ed3\u9762\u8bd5\u9898"}},l={},c=[{value:"Iceberg\u96c6\u6210Hive",id:"iceberg\u96c6\u6210hive",level:2},{value:"\u96c6\u6210Hive\u914d\u7f6e",id:"\u96c6\u6210hive\u914d\u7f6e",level:3},{value:"\u6dfb\u52a0jar\u5305",id:"\u6dfb\u52a0jar\u5305",level:4},{value:"\u914d\u7f6e\u5f00\u542fIceberg",id:"\u914d\u7f6e\u5f00\u542ficeberg",level:4},{value:"\u542f\u52a8Hive MetaStore\u670d\u52a1",id:"\u542f\u52a8hive-metastore\u670d\u52a1",level:4},{value:"\u521b\u5efa\u8868",id:"\u521b\u5efa\u8868",level:3},{value:"\u4f7f\u7528Hive\u9ed8\u8ba4\u7684Catalog",id:"\u4f7f\u7528hive\u9ed8\u8ba4\u7684catalog",level:4},{value:"\u81ea\u5b9a\u4e49HiveCatalog\u540d\u79f0",id:"\u81ea\u5b9a\u4e49hivecatalog\u540d\u79f0",level:4},{value:"\u914d\u7f6eHadoop Catalog",id:"\u914d\u7f6ehadoop-catalog",level:4},{value:"Iceberg\u96c6\u6210Spark",id:"iceberg\u96c6\u6210spark",level:2},{value:"Spark\u4ee3\u7801",id:"spark\u4ee3\u7801",level:3},{value:"Maven\u914d\u7f6e",id:"maven\u914d\u7f6e",level:4},{value:"\u4ee3\u7801\u5b9e\u73b0",id:"\u4ee3\u7801\u5b9e\u73b0",level:4},{value:"DataFrame\u8bfb\u53d6\u8868",id:"dataframe\u8bfb\u53d6\u8868",level:4},{value:"\u67e5\u8be2\u5feb\u7167\u4fe1\u606f",id:"\u67e5\u8be2\u5feb\u7167\u4fe1\u606f",level:4},{value:"\u67e5\u8be2\u5feb\u7167\u8868",id:"\u67e5\u8be2\u5feb\u7167\u8868",level:4},{value:"\u67e5\u8be2\u65f6\u95f4\u6233\u6570\u636e",id:"\u67e5\u8be2\u65f6\u95f4\u6233\u6570\u636e",level:4},{value:"\u56de\u6eda\u5230\u67d0\u4e2a\u5feb\u7167",id:"\u56de\u6eda\u5230\u67d0\u4e2a\u5feb\u7167",level:4},{value:"\u5408\u5e76Compact",id:"\u5408\u5e76compact",level:4},{value:"Spark Sql Shell",id:"spark-sql-shell",level:3},{value:"Hadoop Catalog",id:"hadoop-catalog",level:4},{value:"\u8bfb\u53d6\u6d41\u7a0b",id:"\u8bfb\u53d6\u6d41\u7a0b",level:2},{value:"\u67e5\u8be2\u6700\u65b0\u5feb\u7167\u7684\u6570\u636e",id:"\u67e5\u8be2\u6700\u65b0\u5feb\u7167\u7684\u6570\u636e",level:3},{value:"\u67e5\u8be2\u67d0\u4e2a\u5feb\u7167\u7684\u6570\u636e",id:"\u67e5\u8be2\u67d0\u4e2a\u5feb\u7167\u7684\u6570\u636e",level:3},{value:"\u6839\u636e\u65f6\u95f4\u6233\u67e5\u770b\u67d0\u4e2a\u5feb\u7167\u7684\u6570\u636e",id:"\u6839\u636e\u65f6\u95f4\u6233\u67e5\u770b\u67d0\u4e2a\u5feb\u7167\u7684\u6570\u636e",level:3},{value:"DDL",id:"ddl",level:2},{value:"\u5206\u533a\u8868",id:"\u5206\u533a\u8868",level:3},{value:"\u9690\u5f0f\u5206\u533a",id:"\u9690\u5f0f\u5206\u533a",level:3},{value:"Creat Table as select",id:"creat-table-as-select",level:3},{value:"\u5199\u64cd\u4f5c",id:"\u5199\u64cd\u4f5c",level:2},{value:"MergeINTO",id:"mergeinto",level:3},{value:"INSERT OVERWRIT",id:"insert-overwrit",level:3},{value:"\u52a8\u6001\u5206\u533a\u8986\u76d6",id:"\u52a8\u6001\u5206\u533a\u8986\u76d6",level:4},{value:"\u9759\u6001\u5206\u533a\u8986\u76d6",id:"\u9759\u6001\u5206\u533a\u8986\u76d6",level:4},{value:"Update",id:"update",level:3},{value:"DataFrame API \u5199\u5165Iceberg\u8868",id:"dataframe-api-\u5199\u5165iceberg\u8868",level:3},{value:"Structured Streaming\u5b9e\u65f6\u5199\u5165Iceberg",id:"structured-streaming\u5b9e\u65f6\u5199\u5165iceberg",level:2},{value:"Iceberg\u96c6\u6210Flink",id:"iceberg\u96c6\u6210flink",level:2},{value:"Maven Pom",id:"maven-pom",level:3},{value:"\u521b\u5efa\u8868",id:"\u521b\u5efa\u8868-1",level:3},{value:"\u8bfb\u53d6\u8868",id:"\u8bfb\u53d6\u8868",level:3},{value:"\u5b9e\u65f6\u5199\u5165\u8868",id:"\u5b9e\u65f6\u5199\u5165\u8868",level:3},{value:"SQL API \u64cd\u4f5c",id:"sql-api-\u64cd\u4f5c",level:3},{value:"\u6279\u91cf\u8bfb\u53d6\u8868",id:"\u6279\u91cf\u8bfb\u53d6\u8868",level:4},{value:"\u5b9e\u65f6\u8bfb\u53d6\u8868",id:"\u5b9e\u65f6\u8bfb\u53d6\u8868",level:4},{value:"\u8bfb\u53d6Kafka\u5199\u5165\u8868",id:"\u8bfb\u53d6kafka\u5199\u5165\u8868",level:4}];function d(e){const n={blockquote:"blockquote",code:"code",h2:"h2",h3:"h3",h4:"h4",img:"img",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,r.jsxs)(r.Fragment,{children:[(0,r.jsx)(n.h2,{id:"iceberg\u96c6\u6210hive",children:"Iceberg\u96c6\u6210Hive"}),"\n",(0,r.jsx)(n.h3,{id:"\u96c6\u6210hive\u914d\u7f6e",children:"\u96c6\u6210Hive\u914d\u7f6e"}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:"Hive \u7248\u672c\u4e3a3.1.2, Iceberg\u7248\u672c\u4e3a1.3.0"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"\u6dfb\u52a0jar\u5305",children:"\u6dfb\u52a0jar\u5305"}),"\n",(0,r.jsx)(n.p,{children:"\u5728Hive\u5b89\u88c5\u76ee\u5f55\u4e0b\u6dfb\u52a02\u4e2aJar\u5305"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"iceberg-hive-runtime-1.3.0.jar"}),"\n",(0,r.jsx)(n.li,{children:"libfb303-0.9.3.jar(\u8be5\u5305\u5728Hive3.0\u540e\u88abHive\u79fb\u9664\u4e86\uff0c\u4f46\u662fIceberg\u9700\u8981\u4f7f\u7528\u8fd9\u4e2a\u5305)"}),"\n"]}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"\u542f\u52a8Hive shell,\u6267\u884cadd jar\u547d\u4ee4\u6dfb\u52a0jar\u5305."}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"add jar /opt/hive-3.1.2-bin/auxlib/iceberg-hive-runtime-1.3.0.jar;\n\nadd jar /opt/hive-3.1.2-bin/auxlib/libfb303-0.9.3.jar;\n"})}),"\n",(0,r.jsx)(n.h4,{id:"\u914d\u7f6e\u5f00\u542ficeberg",children:"\u914d\u7f6e\u5f00\u542fIceberg"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"\u914d\u7f6e\u5f00\u542fIceberg"}),"\n"]}),"\n",(0,r.jsx)(n.p,{children:"\u5728Hive\u5ba2\u6237\u7aef$HIVE_HOME/conf/hive-site.xml\u4e2d\u6dfb\u52a0\u5982\u4e0b\u914d\u7f6e\uff1a"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:"<property>\n  <name>iceberg.engine.hive.enabled</name>\n  <value>true</value>\n</property>\n"})}),"\n",(0,r.jsx)(n.p,{children:"\u6216\u8005\u5728Hive Shell\u4e2d\u914d\u7f6e"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"set iceberg.engine.hive.enabled=true;\n"})}),"\n",(0,r.jsx)(n.h4,{id:"\u542f\u52a8hive-metastore\u670d\u52a1",children:"\u542f\u52a8Hive MetaStore\u670d\u52a1"}),"\n",(0,r.jsxs)(n.p,{children:["Iceberg\u9700\u8981\u8fde\u63a5HiveMetaStore\u7528\u6765\u4f5c\u4e3aCatalog\uff0c\u56e0\u6b64\u9700\u8981\u5f00",(0,r.jsx)("br",{}),"  \u542fMetaStore\u670d\u52a1",(0,r.jsx)("br",{}),"  \u5148\u5728hive-site.xml\u4e2d\u914d\u7f6emetastore"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:"<property>\n  <name>hive.metastore.uris</name>\n  <value>thrift://hadoop2:9083</value>\n  <description>Thrift URI for the remote metastore. Used by metastore client to connect to remote metastore.</description>\n</property>\n"})}),"\n",(0,r.jsx)(n.p,{children:"\u542f\u52a8Hive MetaStore\u670d\u52a1"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:" hive --service metastore\n"})}),"\n",(0,r.jsx)(n.h3,{id:"\u521b\u5efa\u8868",children:"\u521b\u5efa\u8868"}),"\n",(0,r.jsx)(n.h4,{id:"\u4f7f\u7528hive\u9ed8\u8ba4\u7684catalog",children:"\u4f7f\u7528Hive\u9ed8\u8ba4\u7684Catalog"}),"\n",(0,r.jsx)(n.p,{children:"\u5982\u679c\u6ca1\u6709\u8bbe\u7f6eiceberg.catalog\u5c5e\u6027\uff0c\u9ed8\u8ba4\u4f7f\u7528HiveCatalog\u6765\u52a0\u8f7d\uff0c\u8fd9\u79cd\u65b9\u5f0f\u5c31\u662f\u8bf4\u5982\u679c\u5728Hive\u4e2d\u521b\u5efaIceberg\u683c\u5f0f\u8868\u65f6\uff0c\u4e0d\u6307\u5b9aiceberg.catalog\u5c5e\u6027\uff0c\u90a3\u4e48\u6570\u636e\u5b58\u50a8\u5728\u5bf9\u5e94\u7684hive warehouse\u8def\u5f84\u4e0b\u3002"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sql",children:"#\u5728Hive\u4e2d\u521b\u5efaiceberg\u683c\u5f0f\u8868\ncreate table test_iceberg_tbl1(\n  id int ,\n  name string,\n  age int) \npartitioned by (dt string) \nstored by 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler';\n"})}),"\n",(0,r.jsx)(n.p,{children:"\u63d2\u5165\u6570\u636e"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sql",children:'insert into test_iceberg_tbl1 values (1,"zs",18,"20211212");\n'})}),"\n",(0,r.jsx)(n.h4,{id:"\u81ea\u5b9a\u4e49hivecatalog\u540d\u79f0",children:"\u81ea\u5b9a\u4e49HiveCatalog\u540d\u79f0"}),"\n",(0,r.jsx)(n.p,{children:"\u8fd9\u79cd\u60c5\u51b5\u5c31\u662f\u8bf4\u5728Hive\u4e2d\u521b\u5efaIceberg\u683c\u5f0f\u8868\u65f6\uff0c\u5982\u679c\u6307\u5b9a\u4e86iceberg.catalog\u5c5e\u6027\u503c\uff0c\u90a3\u4e48\u6570\u636e\u5b58\u50a8\u5728\u6307\u5b9a\u7684catalog\u540d\u79f0\u5bf9\u5e94\u914d\u7f6e\u7684\u76ee\u5f55\u4e0b\u3002\u5176\u5b9e\u81ea\u5b9a\u4e49HiveCatalog\u7684\u6548\u679c\u548c\u9ed8\u8ba4\u7684Hive Catalog\u4e00\u6837,\u6ca1\u4ec0\u4e48\u533a\u522b\u3002"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sql",children:"SET iceberg.catalog.another_hive.type=hive;\nSET iceberg.catalog.another_hive.uri=thrift://hadoop2:9083;\nSET iceberg.catalog.another_hive.clients=10;\nSET iceberg.catalog.another_hive.warehouse=hdfs://hadoop1:9098/warehouse/another_hive;\n"})}),"\n",(0,r.jsx)(n.p,{children:"\u521b\u5efa\u8868"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-plsql",children:"create table another_hive.test_iceberg_tbl4(\n  id int,\n  name string,\n  age int\n)\npartitioned by (dt string)\nstored by 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler'\ntblproperties ('iceberg.catalog'='another_hive');\n"})}),"\n",(0,r.jsx)(n.p,{children:"\u5982\u679c\u8981\u81ea\u5b9a\u4e49\u8868\u7684\u5b58\u50a8\u8def\u5f84\uff0c\u90a3\u4e48\u9700\u8981\u589e\u52a0location\u5c5e\u6027\uff0c\u5982\u4e0b\uff1a"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-plsql",children:"create table test_iceberg_tbl6(\n  id int,\n  name string,\n  age int\n)\npartitioned by (dt string)\nstored by 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler'\nlocation  '/tmp/iceberg/test_iceberg_tbl5'\ntblproperties ('iceberg.catalog'='another_hive');\n"})}),"\n",(0,r.jsx)(n.h4,{id:"\u914d\u7f6ehadoop-catalog",children:"\u914d\u7f6eHadoop Catalog"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-plsql",children:"SET iceberg.catalog.hadoop.type=hadoop;\nSET iceberg.catalog.hadoop.warehouse=hdfs://hadoop1:9098/iceberg/warehouse;\n"})}),"\n",(0,r.jsx)(n.p,{children:"\u521b\u5efa\u8868"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sql",children:"create external table test_iceberg_tbl3(\n  id int,\n  name string,\n  age int\n)\npartitioned by (dt string)\nstored by 'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler'\nlocation 'hdfs://hadoop1:9098/iceberg/warehouse/default/test_iceberg_tbl3'\ntblproperties ('iceberg.catalog'='hadoop');\n"})}),"\n",(0,r.jsxs)(n.p,{children:["\u6ce8\u610f\uff1a\u4ee5\u4e0alocation\u6307\u5b9a\u7684\u8def\u5f84\u5fc5\u987b\u662f\u201ciceberg.catalog.hadoop.warehouse\u201d\u6307\u5b9a\u8def\u5f84\u7684\u5b50\u8def\u5f84,\u683c\u5f0f\u5fc5\u987b\u662f",(0,r.jsx)(n.code,{children:"${iceberg.catalog.hadoop.warehouse}"}),"/ ",(0,r.jsx)(n.code,{children:"${\u5f53\u524d\u5efa\u8868\u4f7f\u7528\u7684hive\u5e93}"}),"/ ",(0,r.jsx)(n.code,{children:"${\u521b\u5efa\u7684\u5f53\u524diceberg\u8868\u540d}"}),"\u3002",(0,r.jsx)("br",{}),(0,r.jsx)(n.strong,{children:"\u67e5\u770b\u8868\u7ed3\u6784"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sql",children:"CREATE EXTERNAL TABLE `test_iceberg_tbl3`(\n  `id` int COMMENT 'from deserializer',\n  `name` string COMMENT 'from deserializer',\n  `age` int COMMENT 'from deserializer',\n  `dt` string COMMENT 'from deserializer')\nROW FORMAT SERDE\n  'org.apache.iceberg.mr.hive.HiveIcebergSerDe'\nSTORED BY\n  'org.apache.iceberg.mr.hive.HiveIcebergStorageHandler'\nWITH SERDEPROPERTIES (\n  'serialization.format'='1')\nLOCATION\n  'hdfs://hadoop1:9098/iceberg/warehouse/default/test_iceberg_tbl3'\nTBLPROPERTIES (\n  'bucketing_version'='2',\n  'external.table.purge'='TRUE',\n  'iceberg.catalog'='hadoop',\n  'table_type'='ICEBERG',\n  'transient_lastDdlTime'='1706019628')\n\n"})}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"\u63d2\u5165\u6570\u636e"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sql",children:'insert into test_iceberg_tbl3 values (3,"ww",20,"20211213");\n'})}),"\n",(0,r.jsx)(n.h2,{id:"iceberg\u96c6\u6210spark",children:"Iceberg\u96c6\u6210Spark"}),"\n",(0,r.jsx)(n.h3,{id:"spark\u4ee3\u7801",children:"Spark\u4ee3\u7801"}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:"\u7248\u672c\uff1aSpark 3.3.2 Iceberg 1.4.3"}),"\n"]}),"\n",(0,r.jsx)(n.h4,{id:"maven\u914d\u7f6e",children:"Maven\u914d\u7f6e"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:"<properties>\n  <spark.version>3.3.2</spark.version>\n  <hive.version>3.1.2</hive.version>\n  <scala.version>2.12</scala.version>\n  <iceberg.version>1.4.3</iceberg.version>\n</properties>\n<dependencies>\n  <dependency>\n    <groupId>org.apache.spark</groupId>\n    <artifactId>spark-core_2.12</artifactId>\n    <version>${spark.version}</version>\n  </dependency>\n  <dependency>\n    <groupId>org.apache.spark</groupId>\n    <artifactId>spark-sql_2.12</artifactId>\n    <version>${spark.version}</version>\n  </dependency>\n  <dependency>\n    <groupId>org.apache.hive</groupId>\n    <artifactId>hive-jdbc</artifactId>\n    <version>3.1.2</version>\n  </dependency>\n  <dependency>\n    <groupId>log4j</groupId>\n    <artifactId>log4j</artifactId>\n    <version>1.2.17</version>\n  </dependency>\n  <dependency>\n    <groupId>org.apache.spark</groupId>\n    <artifactId>spark-hive_2.12</artifactId>\n    <version>${spark.version}</version>\n  </dependency>\n  <dependency>\n    <groupId>org.apache.hadoop</groupId>\n    <artifactId>hadoop-client</artifactId>\n    <version>3.2.0</version>\n  </dependency>\n  <dependency>\n    <groupId>org.apache.hadoop</groupId>\n    <artifactId>hadoop-common</artifactId>\n    <version>2.7.2</version>\n  </dependency>\n  <dependency>\n    <groupId>org.apache.iceberg</groupId>\n    <artifactId>iceberg-spark</artifactId>\n    <version>${iceberg.version}</version>\n  </dependency>\n  <dependency>\n    <groupId>org.apache.iceberg</groupId>\n    <artifactId>iceberg-spark-runtime-3.3_2.12</artifactId>\n    <version>${iceberg.version}</version>\n  </dependency>\n  <dependency>\n    <groupId>org.apache.avro</groupId>\n    <artifactId>avro</artifactId>\n    <version>1.10.2</version>\n  </dependency>\n  <dependency>\n    <groupId>org.apache.parquet</groupId>\n    <artifactId>parquet-hadoop</artifactId>\n    <version>1.12.2</version>\n  </dependency>\n  <dependency>\n    <groupId>mysql</groupId>\n    <artifactId>mysql-connector-java</artifactId>\n    <version>5.1.37</version>\n  </dependency>\n</dependencies>\n"})}),"\n",(0,r.jsx)(n.h4,{id:"\u4ee3\u7801\u5b9e\u73b0",children:"\u4ee3\u7801\u5b9e\u73b0"}),"\n",(0,r.jsxs)(n.ol,{children:["\n",(0,r.jsx)(n.li,{children:"\u914d\u7f6eHive catalog"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-scala",children:'val ss: SparkSession = SparkSession.builder().master("local[*]")\n.appName("SparkIcebergDemo")\n.config("spark.sql.catalog.hive_prod", "org.apache.iceberg.spark.SparkCatalog")\n.config("spark.sql.catalog.hive_prod.type", "hive")\n.config("spark.sql.catalog.hive_prod.uri", "thrift://hadoop2:9083")\n.getOrCreate()\n'})}),"\n",(0,r.jsxs)(n.ol,{start:"2",children:["\n",(0,r.jsx)(n.li,{children:"\u914d\u7f6eHadoop Catalog"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-scala",children:'val ss: SparkSession = SparkSession.builder().master("local[*]")\n.appName("SparkIcebergDemo")\n.config("spark.sql.catalog.hadoop_prod", "org.apache.iceberg.spark.SparkCatalog")\n.config("spark.sql.catalog.hadoop_prod.type", "hadoop")\n.config("spark.sql.catalog.hadoop_prod.warehouse", "hdfs://hadoop1:9098/warehouse/sparkiceberg")\n.getOrCreate()\n'})}),"\n",(0,r.jsx)(n.p,{children:"\u6ce8\u610f,window\u9700\u8981\u914d\u7f6e"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-scala",children:'System.setProperty("hadoop.home.dir","D:\\\\aaa\\\\hadoop-2.7.6\\\\hadoop-2.7.6");\n//\u5fc5\u987b\u8981\u8bbe\u7f6e,\u5426\u5219spark\u4f1a\u5199hive\u4f1a\u62a5HDFS\u6743\u9650\u95ee\u9898\nSystem.setProperty("HADOOP_USER_NAME","root");\n'})}),"\n",(0,r.jsxs)(n.ol,{start:"2",children:["\n",(0,r.jsx)(n.li,{children:"\u521b\u5efa\u8868"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-scala",children:'ss.sql(\n  """\n             |CREATE TABLE catalog_name.default.table1 (id bigint, data string) USING iceberg\n           """.stripMargin)\n'})}),"\n",(0,r.jsx)(n.h4,{id:"dataframe\u8bfb\u53d6\u8868",children:"DataFrame\u8bfb\u53d6\u8868"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-scala",children:'val ss: SparkSession = SparkSession.builder().master("local[*]")\n.appName("SparkIcebergDemo")\n.config("spark.sql.catalog.hadoop_prod", "org.apache.iceberg.spark.SparkCatalog")\n.config("spark.sql.catalog.hadoop_prod.type", "hadoop")\n.config("spark.sql.catalog.hadoop_prod.warehouse", "hdfs://hadoop1:9098/warehouse/sparkiceberg")\n.getOrCreate()\n\n\nss.read.format("iceberg").load("hadoop_prod.xxx.tb4").show()\n'})}),"\n",(0,r.jsx)(n.p,{children:"\u7b2c\u4e8c\u79cd\u65b9\u5f0f"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-scala",children:'ss.table("hadoop_prod.xxx.tb4").show()\n'})}),"\n",(0,r.jsx)(n.h4,{id:"\u67e5\u8be2\u5feb\u7167\u4fe1\u606f",children:"\u67e5\u8be2\u5feb\u7167\u4fe1\u606f"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-scala",children:" select * from hadoop_prod.xxx.tb4.snapshots\n"})}),"\n",(0,r.jsx)(n.h4,{id:"\u67e5\u8be2\u5feb\u7167\u8868",children:"\u67e5\u8be2\u5feb\u7167\u8868"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-scala",children:'ss.read.format("iceberg")\n.option("snapshot-id",6268558238709300005L)\n.load("hadoop_prod.xxx.tb4").show()\n'})}),"\n",(0,r.jsx)(n.h4,{id:"\u67e5\u8be2\u65f6\u95f4\u6233\u6570\u636e",children:"\u67e5\u8be2\u65f6\u95f4\u6233\u6570\u636e"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-scala",children:'ss.read.format("iceberg")\n.option("as-of-timestamp",1706231158560L)\n.load("hadoop_prod.xxx.tb4").show()\n'})}),"\n",(0,r.jsx)(n.h4,{id:"\u56de\u6eda\u5230\u67d0\u4e2a\u5feb\u7167",children:"\u56de\u6eda\u5230\u67d0\u4e2a\u5feb\u7167"}),"\n",(0,r.jsxs)(n.p,{children:["Spark DataFrame \u4e0d\u652f\u6301\u56de\u6eda\u5230\u5feb\u7167",(0,r.jsx)("br",{}),"Spark3.x\u540esql\u7684\u5f62\u5f0f\u53ef\u4ee5\u56de\u6eda",(0,r.jsx)("br",{}),"\u4f46\u662f\u6211\u5728\u6d4b\u8bd5\u7684\u65f6\u5019\u5e76\u6ca1\u6709\u6210\u529f\u3002\u4f7f\u7528Java API \u53ef\u4ee5\u5b9e\u73b0\u5feb\u7167\u56de\u6eda"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:'System.setProperty("hadoop.home.dir","D:\\\\aaa\\\\hadoop-2.7.6\\\\hadoop-2.7.6");\n//\u5fc5\u987b\u8981\u8bbe\u7f6e,\u5426\u5219spark\u4f1a\u5199hive\u4f1a\u62a5HDFS\u6743\u9650\u95ee\u9898\nSystem.setProperty("HADOOP_USER_NAME","root");\nConfiguration conf = new Configuration();\nHadoopCatalog catalog = new HadoopCatalog(conf, "hdfs://hadoop1:9098/warehouse/sparkiceberg");\ncatalog.setConf(conf);\nTable table = catalog.loadTable(TableIdentifier.of("xxx", "person_info"));\ntable.manageSnapshots().rollbackTo(7076891289188705084L).commit();\n'})}),"\n",(0,r.jsx)(n.h4,{id:"\u5408\u5e76compact",children:"\u5408\u5e76Compact"}),"\n",(0,r.jsx)(n.p,{children:"\u5408\u5e76\u5c0f\u6587\u4ef6\u6570\u636e,Iceberg\u5408\u5e76\u5c0f\u6587\u4ef6\u65f6\u5e76\u4e0d\u4f1a\u5220\u9664\u88ab\u5408\u5e76\u7684\u6587\u4ef6\uff0cCompact\u662f\u5c06\u5c0f\u6587\u4ef6\u5408\u5e76\u6210\u5927\u6587\u4ef6\u5e76\u521b\u5efa\u65b0\u7684Snapshot"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:'//2) \u5408\u5e76\u5c0f\u6587\u4ef6\u6570\u636e,Iceberg\u5408\u5e76\u5c0f\u6587\u4ef6\u65f6\u5e76\u4e0d\u4f1a\u5220\u9664\u88ab\u5408\u5e76\u7684\u6587\u4ef6\uff0c\n// Compact\u662f\u5c06\u5c0f\u6587\u4ef6\u5408\u5e76\u6210\u5927\u6587\u4ef6\u5e76\u521b\u5efa\u65b0\u7684Snapshot\u3002\n// \u5982\u679c\u8981\u5220\u9664\u6587\u4ef6\u9700\u8981\u901a\u8fc7Expire Snapshots\u6765\u5b9e\u73b0,targetSizeInBytes \u6307\u5b9a\u5408\u5e76\u540e\u7684\u6bcf\u4e2a\u6587\u4ef6\u5927\u5c0f\nConfiguration conf = new Configuration();\nHadoopCatalog catalog = new HadoopCatalog(conf,"hdfs://hadoop1:9098/warehouse/sparkiceberg");\nTable table = catalog.loadTable(TableIdentifier.of("xxx","person_info"));\nSparkSession sparkSession = SparkSession.builder().master("local[1]")\n.appName("SparkIcebergDemo")\n.config("spark.sql.catalog.hadoop_prod", "org.apache.iceberg.spark.SparkCatalog")\n.config("spark.sql.catalog.hadoop_prod.type", "hadoop")\n.config("spark.sql.catalog.hadoop_prod.warehouse", "hdfs://hadoop1:9098/warehouse/sparkiceberg")\n.getOrCreate();\nSparkActions sparkActions = SparkActions.get(sparkSession);\nRewriteDataFilesSparkAction rewriteDataFilesSparkAction = sparkActions.rewriteDataFiles(table);\nrewriteDataFilesSparkAction.execute();\n'})}),"\n",(0,r.jsx)(n.h3,{id:"spark-sql-shell",children:"Spark Sql Shell"}),"\n",(0,r.jsx)(n.h4,{id:"hadoop-catalog",children:"Hadoop Catalog"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:"bin/spark-sql --packages org.apache.iceberg:iceberg-spark-runtime-3.2_2.12:1.4.3\\\n    --conf spark.sql.catalog.hadoop_prod=org.apache.iceberg.spark.SparkCatalog \\\n\t  --conf spark.sql.extensions=org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions \\\n    --conf spark.sql.catalog.hadoop_prod.type=hadoop \\\n    --conf spark.sql.catalog.hadoop_prod.warehouse=hdfs://hadoop1:9098/warehouse/sparkiceberg\n"})}),"\n",(0,r.jsx)(n.h2,{id:"\u8bfb\u53d6\u6d41\u7a0b",children:"\u8bfb\u53d6\u6d41\u7a0b"}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.img,{src:"https://cdn.nlark.com/yuque/0/2024/png/390265/1706105747326-4ba93cce-ab82-440b-904c-95525b02728e.png#averageHue=%23fcd6d2&clientId=u651027a0-ca68-4&from=paste&id=u244dec96&originHeight=533&originWidth=1080&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=u21d572fb-827a-4978-86c9-468c20eebc4&title=",alt:""}),(0,r.jsx)("br",{}),"\u5047\u8bbe\u6211\u4eec\u7684\u8868\u662f\u5b58\u50a8\u5728 Hive \u7684 MetaStore \u91cc\u9762\u7684\uff0c\u8868\u540d\u4e3a iteblog\uff0c\u5e76\u4e14\u6570\u636e\u7684\u7ec4\u7ec7\u7ed3\u6784\u5982\u4e0a\u5982\u6240\u793a\u3002"]}),"\n",(0,r.jsx)(n.h3,{id:"\u67e5\u8be2\u6700\u65b0\u5feb\u7167\u7684\u6570\u636e",children:"\u67e5\u8be2\u6700\u65b0\u5feb\u7167\u7684\u6570\u636e"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"\u901a\u8fc7\u6570\u636e\u5e93\u540d\u548c\u8868\u540d\uff0c\u4ece Hive \u7684 MetaStore \u91cc\u9762\u62ff\u5230\u8868\u7684\u4fe1\u606f\u3002\u4ece\u8868\u7684\u5c5e\u6027\u91cc\u9762\u5176\u5b9e\u53ef\u4ee5\u62ff\u5230 metadata_location \u5c5e\u6027\uff0c\u901a\u8fc7\u8fd9\u4e2a\u5c5e\u6027\u53ef\u4ee5\u62ff\u5230 iteblog \u8868\u7684 Iceberg \u7684 metadata \u76f8\u5173\u8def\u5f84\uff0c\u8fd9\u4e2a\u4e5f\u5c31\u662f\u4e0a\u56fe\u6b65\u9aa4\u2460\u7684 /user/iteblog/metadata/2.metadata.json\u3002"}),"\n",(0,r.jsx)(n.li,{children:"\u89e3\u6790 /user/iteblog/metadata/2.metadata.json \u6587\u4ef6\uff0c\u91cc\u9762\u53ef\u4ee5\u62ff\u5230\u5f53\u524d\u8868\u7684\u5feb\u7167 id\uff08current-snapshot-id\uff09\uff0c\u4ee5\u53ca\u8fd9\u5f20\u8868\u7684\u6240\u6709\u5feb\u7167\u4fe1\u606f\uff0c\u4e5f\u5c31\u662f JOSN \u4fe1\u606f\u91cc\u9762\u7684 snapshots \u6570\u7ec4\u5bf9\u5e94\u7684\u503c\u3002\u4ece\u4e0a\u56fe\u53ef\u4ee5\u770b\u51fa\uff0c\u5f53\u524d\u8868\u6709\u4e24\u4e2a\u5feb\u7167\uff0cid \u5206\u522b\u4e3a 1 \u548c 2\u3002\u5feb\u7167 1 \u5bf9\u5e94\u7684\u6e05\u5355\u5217\u8868\u6587\u4ef6\u4e3a /user/iteblog/metastore/snap-1.avro\uff1b\u5feb\u7167 2 \u5bf9\u5e94\u7684\u6e05\u5355\u5217\u8868\u6587\u4ef6\u4e3a /user/iteblog/metastore/snap-2.avro\u3002"}),"\n",(0,r.jsx)(n.li,{children:"\u5982\u679c\u6211\u4eec\u60f3\u8bfb\u53d6\u8868\u7684\u6700\u65b0\u5feb\u7167\u6570\u636e\uff0c\u4ece current-snapshot-id \u53ef\u77e5\uff0c\u5f53\u524d\u6700\u65b0\u5feb\u7167\u7684 ID \u7b49\u4e8e 2\uff0c\u6240\u4ee5\u6211\u4eec\u53ea\u9700\u8981\u89e3\u6790 /user/iteblog/metastore/snap-2.avro \u6e05\u5355\u5217\u8868\u6587\u4ef6\u5373\u53ef\u3002\u4ece\u4e0a\u56fe\u53ef\u4ee5\u770b\u51fa\uff0csnap-2.avro \u8fd9\u4e2a\u6e05\u5355\u5217\u8868\u6587\u4ef6\u91cc\u9762\u6709\u4e24\u4e2a\u6e05\u5355\u6587\u4ef6\uff0c\u5206\u522b\u4e3a /user/iteblog/metadata/3.avro \u548c /user/iteblog/metadata/2.avro\u3002\u6ce8\u610f\uff0c\u9664\u4e86\u6e05\u5355\u6587\u4ef6\u7684\u8def\u5f84\u4fe1\u606f\uff0c\u8fd8\u6709 added_data_files_count\u3001existing_data_files_count \u4ee5\u53ca deleted_data_files_count \u4e09\u4e2a\u5c5e\u6027\u3002Iceberg \u5176\u5b9e\u662f\u6839\u636e deleted_data_files_count \u5927\u4e8e 0 \u6765\u5224\u65ad\u5bf9\u5e94\u7684\u6e05\u5355\u6587\u4ef6\u91cc\u9762\u662f\u4e0d\u662f\u88ab\u5220\u9664\u7684\u6570\u636e\u3002\u7531\u4e8e\u4e0a\u56fe /user/iteblog/metadata/2.avro \u6e05\u5355\u6587\u4ef6\u7684 deleted_data_files_count \u5927\u4e8e 0 \uff0c\u6240\u4ee5\u8bfb\u6570\u636e\u7684\u65f6\u5019\u5c31\u65e0\u9700\u8bfb\u8fd9\u4e2a\u6e05\u5355\u6587\u4ef6\u91cc\u9762\u5bf9\u5e94\u7684\u6570\u636e\u6587\u4ef6\u3002\u5728\u8fd9\u4e2a\u573a\u666f\u4e0b\uff0c\u8bfb\u53d6\u6700\u65b0\u5feb\u7167\u6570\u636e\u53ea\u9700\u8981\u770b\u4e0b /user/iteblog/metadata/3.avro \u6e05\u5355\u6587\u4ef6\u91cc\u9762\u5bf9\u5e94\u7684\u6570\u636e\u6587\u4ef6\u5373\u53ef\u3002"}),"\n",(0,r.jsx)(n.li,{children:"\u8fd9\u65f6\u5019 Iceberg \u4f1a\u89e3\u6790 /user/iteblog/metadata/3.avro \u6e05\u5355\u6587\u4ef6\uff0c\u91cc\u9762\u5176\u5b9e\u5c31\u53ea\u6709\u4e00\u884c\u6570\u636e\uff0c\u4e5f\u5c31\u662f /user/iteblog/data/4.parquet\uff0c\u6240\u4ee5\u6211\u4eec\u8bfb iteblog \u6700\u65b0\u7684\u6570\u636e\u5176\u5b9e\u53ea\u9700\u8981\u8bfb /user/iteblog/data/4.parquet \u6570\u636e\u6587\u4ef6\u5c31\u53ef\u4ee5\u4e86\u3002\u6ce8\u610f\uff0c\u4e0a\u9762 /user/iteblog/data/2.avro \u6587\u4ef6\u91cc\u9762\u5bf9\u5e94\u7684\u5185\u5bb9\u4e3a"}),"\n"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-json",children:'{"status":2,"data_file":{"file_path":"/user/iteblog/data/3.parquet"}}\n{"status":2,"data_file":{"file_path":"/user/iteblog/data/2.parquet"}}{\u201cstatus":2,"data_file":{"file_path":"/user/iteblog/data/1.parquet"}}\n'})}),"\n",(0,r.jsxs)(n.p,{children:["\u5176\u4e2d\u7684 status = 2 \u4ee3\u8868 DELETED\uff0c\u4e5f\u5c31\u662f\u5220\u9664\uff0c\u4e5f\u5370\u8bc1\u4e86\u8bfb\u6700\u65b0\u5feb\u7167\u7684\u6570\u636e\u5176\u5b9e\u4e0d\u7528\u8bfb /user/iteblog/data/2.avro \u6e05\u5355\u6587\u4ef6\u7684\u6570\u636e\u6587\u4ef6\u3002\u800c /user/iteblog/data/3.avro \u6e05\u5355\u6587\u4ef6\u91cc\u9762\u5b58\u50a8\u7684\u5185\u5bb9\u4e3a ",(0,r.jsx)(n.code,{children:'{"status":1,"data_file":{"file_path":"/user/iteblog/data/4.parquet"}}\uff0c\u5176 status = 1'}),"\uff0c\u4ee3\u8868 ADDED\uff0c\u4e5f\u5c31\u662f\u65b0\u589e\u7684\u6587\u4ef6\uff0c\u6240\u4ee5\u5f97\u8bfb\u53d6\u3002"]}),"\n",(0,r.jsx)(n.h3,{id:"\u67e5\u8be2\u67d0\u4e2a\u5feb\u7167\u7684\u6570\u636e",children:"\u67e5\u8be2\u67d0\u4e2a\u5feb\u7167\u7684\u6570\u636e"}),"\n",(0,r.jsxs)(n.p,{children:["Apache Iceberg \u652f\u6301\u67e5\u8be2\u5386\u53f2\u4e0a\u4efb\u4f55\u65f6\u523b\u7684\u5feb\u7167\uff0c\u5728\u67e5\u8be2\u7684\u65f6\u5019\u53ea\u9700\u8981\u6307\u5b9a snapshot-id \u5c5e\u6027\u5373\u53ef\uff0c\u6bd4\u5982\u6211\u4eec\u60f3\u67e5\u8be2\u4e0a\u9762 snapshot-id \u4e3a 1 \u7684\u6570\u636e\uff0c\u53ef\u4ee5\u5728 Spark \u4e2d\u8fd9\u4e48\u5199\uff1a",(0,r.jsx)("br",{}),(0,r.jsx)(n.code,{children:'spark.read .option("snapshot-id", 1L) .format("iceberg") .load("path/to/table")'})," \u4e0b\u9762\u662f\u8bfb\u53d6\u6307\u5b9a\u5feb\u7167\u7684\u56fe\u793a",(0,r.jsx)("br",{}),(0,r.jsx)(n.img,{src:"https://cdn.nlark.com/yuque/0/2024/png/390265/1706105948937-3f07fa5e-2bb4-4a69-b94e-67fbb7a18b84.png#averageHue=%23fcdbd8&clientId=u651027a0-ca68-4&from=paste&id=ue6857bc0&originHeight=534&originWidth=1080&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=u90932cf2-9ed6-4b98-a9f7-8987305ea83&title=",alt:""}),(0,r.jsx)("br",{}),"\u4ece\u4e0a\u56fe\u53ef\u4ee5\u770b\u51fa\uff0c\u548c\u8bfb\u53d6\u6700\u65b0\u5feb\u7167\u6570\u636e\u4e0d\u4e00\u6837\u7684\u5730\u65b9\u662f\u4e0a\u56fe\u4e2d\u7684\u7b2c\u4e09\u6b65\u3002\u7531\u4e8e\u6211\u4eec\u6307\u5b9a\u4e86 snapshot-id = 1\uff0c\u6240\u4ee5 Iceberg \u4f1a\u8bfb\u53d6\u4e0a\u9762\u7b2c\u4e8c\u6b65\u767d\u8272\u7684\u90e8\u5206\uff0c\u53ef\u4ee5\u77e5\u9053\uff0csnapshot-id = 1 \u5bf9\u5e94\u7684\u6e05\u5355\u5217\u8868\u6587\u4ef6\u4e3a /user/iteblog/metastore/snap-1.avro\u3002\u8fd9\u65f6\u5019\u8bfb\u51fa\u6e05\u5355\u5217\u8868\u91cc\u9762\u7684\u6587\u4ef6\uff0c\u5176\u5b9e\u5c31\u53ea\u6709\u4e00\u884c\u6570\u636e\uff0c\u5bf9\u5e94\u7684\u6e05\u5355\u6587\u4ef6\u4e3a /user/iteblog/metadata/1.avro\uff0c\u5176\u4e2d added_data_files_count \u4e3a 3\u3002",(0,r.jsx)("br",{}),"\u4e0b\u4e00\u6b65\u6211\u4eec\u8bfb\u53d6 /user/iteblog/metadata/1.avro \u6e05\u5355\u6587\u4ef6\uff0c\u53ef\u4ee5\u770b\u5230\u91cc\u9762\u6709\u4e09\u4e2a\u6570\u636e\u6587\u4ef6\u8def\u5f84\uff0c\u8fd9\u4e9b\u6570\u636e\u6587\u4ef6\u5c31\u662f snapshot-id = 1 \u7684\u6570\u636e\u3002"]}),"\n",(0,r.jsx)(n.h3,{id:"\u6839\u636e\u65f6\u95f4\u6233\u67e5\u770b\u67d0\u4e2a\u5feb\u7167\u7684\u6570\u636e",children:"\u6839\u636e\u65f6\u95f4\u6233\u67e5\u770b\u67d0\u4e2a\u5feb\u7167\u7684\u6570\u636e"}),"\n",(0,r.jsx)(n.p,{children:"Iceberg \u8fd8\u652f\u6301\u901a\u8fc7 as-of-timestamp \u53c2\u6570\u6307\u5b9a\u65f6\u95f4\u6233\u6765\u8bfb\u53d6\u67d0\u4e2a\u5feb\u7167\u7684\u6570\u636e\u3002\u5982\u4e0b\u6240\u793a\uff1a"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-scala",children:'spark.read.option("as-of-timestamp","12346").format("iceberg").load("path/to/table")\n'})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.img,{src:"https://cdn.nlark.com/yuque/0/2024/png/390265/1706106006170-7c0ff5bc-9b1b-4312-8d8c-20885b4cd7be.png#averageHue=%23fdd9d6&clientId=u651027a0-ca68-4&from=paste&id=ud6f371f5&originHeight=580&originWidth=1080&originalType=url&ratio=1&rotation=0&showTitle=false&status=done&style=none&taskId=uf47468df-0b14-417f-9448-86f10bd292e&title=",alt:""}),(0,r.jsx)("br",{}),"\u6211\u4eec\u6ce8\u610f\u4e0a\u9762\u56fe\u4e2d\u7b2c\u4e8c\u6b65\u91cc\u9762\u7684 JSON \u6570\u636e\u91cc\u9762\u6709\u4e2a snapshot-log \u6570\u7ec4\uff0c\u5982\u4e0b\uff1a"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-scala",children:'"snapshot-log":[{"timestamp-ms":12345,"snapshot-id":1},{"timestamp-ms":23456,"snapshot-id":2}]\n'})}),"\n",(0,r.jsxs)(n.p,{children:["\u6bcf\u4e2a\u5217\u8868\u91cc\u9762\u90fd\u6709\u4e2a timestamp-ms \u5c5e\u6027\u548c snapshot-id \u5c5e\u6027\uff0c\u5e76\u4e14\u662f\u6309\u7167 timestamp-ms \u5347\u5e8f\u7684\u3002\u5728 Iceberg \u5185\u90e8\u5b9e\u73b0\u4e2d\uff0c\u5b83\u4f1a\u5c06 as-of-timestamp \u6307\u5b9a\u7684\u65f6\u95f4\u548c snapshot-log \u6570\u7ec4\u91cc\u9762\u6bcf\u4e2a\u5143\u7d20\u7684 timestamp-ms \u8fdb\u884c\u6bd4\u8f83\uff0c\u627e\u51fa\u6700\u540e\u4e00\u4e2a\u6ee1\u8db3 ",(0,r.jsx)(n.code,{children:"timestamp-ms <= as-of-timestamp"})," \u5bf9\u5e94\u7684 snapshot-id\u3002",(0,r.jsx)("br",{}),"\u7531\u4e8e as-of-timestamp=12346 \u6bd4 12345 \u65f6\u95f4\u6233\u5927\uff0c\u4f46\u662f\u6bd4 23456 \u5c0f\uff0c\u6240\u4ee5\u4f1a\u53d6 snapshot-id = 1\uff0c\u4e5f\u5c31\u662f\u62ff\u5230 snapshot-id = 1 \u7684\u5feb\u7167\u6570\u636e\u3002\u5269\u4e0b\u7684\u6570\u636e\u67e5\u8be2\u6b65\u9aa4\u548c\u5728\u67e5\u8be2\u4e2d\u6307\u5b9a snapshot-id \u662f\u4e00\u81f4\u7684\u3002"]}),"\n",(0,r.jsx)(n.h2,{id:"ddl",children:"DDL"}),"\n",(0,r.jsx)(n.h3,{id:"\u5206\u533a\u8868",children:"\u5206\u533a\u8868"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"\u6b63\u5e38\u5206\u533a\u8868"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-plsql",children:"CREATE TABLE prod.db.sample (\n  id bigint,\n  data string,\n  category string)\nUSING iceberg\nPARTITIONED BY (category)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"\u9690\u5f0f\u5206\u533a",children:"\u9690\u5f0f\u5206\u533a"}),"\n",(0,r.jsx)(n.p,{children:(0,r.jsx)(n.strong,{children:"\u521b\u5efa\u9690\u5f0f\u5206\u533a\u8868"})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-scala",children:'ss.sql(\n  """\n              |CREATE TABLE  if  not exists hadoop_prod.xxx.tb3 (\n              |    id bigint,\n              |    data string,\n              |    category string,\n              |    ts timestamp)\n              |USING iceberg\n              |PARTITIONED BY (years(ts))\n            """.stripMargin)\n\n'})}),"\n",(0,r.jsx)(n.p,{children:"Supported transformations are:"}),"\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"year(ts): partition by year"}),"\n",(0,r.jsx)(n.li,{children:"month(ts): partition by month"}),"\n",(0,r.jsx)(n.li,{children:"day(ts) or date(ts): equivalent to dateint partitioning"}),"\n",(0,r.jsx)(n.li,{children:"hour(ts) or date_hour(ts): equivalent to dateint and hour partitioning"}),"\n",(0,r.jsx)(n.li,{children:"bucket(N, col): partition by hashed value mod N buckets"}),"\n",(0,r.jsxs)(n.li,{children:["truncate(L, col): partition by value truncated to L","\n",(0,r.jsxs)(n.ul,{children:["\n",(0,r.jsx)(n.li,{children:"Strings are truncated to the given length"}),"\n",(0,r.jsx)(n.li,{children:"Integers and longs truncate to bins: truncate(10, i) produces partitions 0, 10, 20, 30, \u2026"}),"\n"]}),"\n"]}),"\n"]}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"\u63d2\u5165\u9690\u5f0f\u5206\u533a\u6570\u636e"}),(0,r.jsx)("br",{}),"\u6ce8\u610f\u9700\u8981cast\u8f6c\u6362\u6570\u636e\u683c\u5f0f"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sql",children:"        ss.sql(\n          \"\"\"\n          |insert into hadoop_prod.xxx.tb3 values\n          |(1,'zhangsan','sh',cast(1706193103 as timestamp)),\n          |(2,'lisi','sz',cast(1674657103 as timestamp)),\n          |(3,'wangwu','wh',cast(1706193103 as timestamp)),\n          |(4,'zhaoliu','lz',cast(1674657103 as timestamp))\n          \"\"\".stripMargin)\n"})}),"\n",(0,r.jsx)(n.h3,{id:"creat-table-as-select",children:"Creat Table as select"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:'        ss.sql(\n            """\n              |CREATE TABLE  if  not exists hadoop_prod.xxx.tb4\n              |USING iceberg\n              |select * from hadoop_prod.xxx.tb3\n            """.stripMargin)\n\n'})}),"\n",(0,r.jsx)(n.h2,{id:"\u5199\u64cd\u4f5c",children:"\u5199\u64cd\u4f5c"}),"\n",(0,r.jsx)(n.h3,{id:"mergeinto",children:"MergeINTO"}),"\n",(0,r.jsxs)(n.p,{children:["Spark 3 \u6dfb\u52a0\u4e86\u5bf9 MERGE INTO \u67e5\u8be2\u7684\u652f\u6301\uff0c\u8be5\u67e5\u8be2\u53ef\u8868\u8fbe\u884c\u7ea7\u66f4\u65b0\u3002Iceberg \u901a\u8fc7\u91cd\u5199\u5305\u542b\u9700\u8981\u5728 overwrite \u63d0\u4ea4\u4e2d\u66f4\u65b0\u7684\u884c\u7684\u6570\u636e\u6587\u4ef6\u6765\u652f\u6301 MERGE INTO\u3002\u5efa\u8bae\u4f7f\u7528 MERGE INTO\uff0c\u800c\u4e0d\u662f INSERT OVERWRITE\uff0c\u56e0\u4e3a Iceberg \u53ea\u80fd\u66ff\u6362\u53d7\u5f71\u54cd\u7684\u6570\u636e\u6587\u4ef6\uff0c\u800c\u4e14\u5982\u679c\u8868\u7684\u5206\u533a\u53d1\u751f\u53d8\u5316\uff0c\u52a8\u6001\u8986\u76d6\u6240\u8986\u76d6\u7684\u6570\u636e\u4e5f\u53ef\u80fd\u53d1\u751f\u53d8\u5316\u3002",(0,r.jsx)("br",{}),"MERGE INTO \u4f7f\u7528\u6765\u81ea\u53e6\u4e00\u4e2a\u67e5\u8be2\uff08\u79f0\u4e3a\u6e90\u67e5\u8be2\uff09\u7684\u4e00\u7ec4\u66f4\u65b0\u6765\u66f4\u65b0\u4e00\u4e2a\u8868\uff08\u79f0\u4e3a\u76ee\u6807\u8868\uff09\u3002\u4f7f\u7528 ON \u5b50\u53e5\u53ef\u4ee5\u627e\u5230\u76ee\u6807\u8868\u4e2d\u67d0\u884c\u7684\u66f4\u65b0\uff0c\u8be5\u5b50\u53e5\u7c7b\u4f3c\u4e8e\u8fde\u63a5\u6761\u4ef6.",(0,r.jsx)("br",{}),(0,r.jsx)(n.strong,{children:"\u8bed\u6cd5"})]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-sql",children:"MERGE INTO prod.db.target t   -- a target table\nUSING (SELECT ...) s          -- the source updates\nON t.id = s.id                -- condition to find updates for target rows\nWHEN ...\n"})}),"\n",(0,r.jsxs)(n.p,{children:[(0,r.jsx)(n.strong,{children:"\u4ee3\u7801\u5b9e\u4f8b"}),(0,r.jsx)("br",{})," \u6ce8\u610f\u4e0b\u9762\u7684\u4ee3\u7801\u4e2d\u5fc5\u987b\u52a0\uff0c\u5426\u5219\u4f1a\u62a5\u9519",(0,r.jsx)(n.code,{children:'Exception in thread "main" java.lang.UnsupportedOperationException: MERGE INTO TABLE is not supported temporarily'}),"."]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-scala",children:'.config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-scala",children:'val spark: SparkSession = SparkSession.builder().master("local[1]")\n.appName("SparkIcebergDemo")\n.config("spark.sql.catalog.hadoop_prod", "org.apache.iceberg.spark.SparkCatalog")\n.config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")\n.config("spark.sql.catalog.hadoop_prod.type", "hadoop")\n.config("spark.sql.catalog.hadoop_prod.warehouse", "hdfs://hadoop1:9098/warehouse/sparkiceberg")\n.getOrCreate()\n\n//\u521b\u5efa\u4e00\u5f20\u8868 a \uff0c\u5e76\u63d2\u5165\u6570\u636e\nspark.sql(\n  """\n    |create table  hadoop_prod.default.a (id int,name string,age int) using iceberg\n  """.stripMargin)\nspark.sql(\n  """\n    |insert into hadoop_prod.default.a values (1,"zs",18),(2,"ls",19),(3,"ww",20)\n  """.stripMargin)\n//\u521b\u5efa\u53e6\u5916\u4e00\u5f20\u8868b ,\u5e76\u63d2\u5165\u6570\u636e\nspark.sql(\n  """\n    |create table  hadoop_prod.default.b (id int,name string,age int,tp string) using iceberg\n  """.stripMargin)\nspark.sql(\n  """\n    |insert into hadoop_prod.default.b values (1,"zs",30,"delete"),(2,"\u674e\u56db",31,"update"),(4,"\u738b\u4e94",32,"add")\n  """.stripMargin)\n\n//\u5c06\u8868b \u4e2d\u4e0e\u8868a\u4e2d\u76f8\u540cid\u7684\u6570\u636e\u66f4\u65b0\u5230\u8868a,\u8868a\u4e2d\u6ca1\u6709\u8868b\u4e2d\u6709\u7684id\u5bf9\u5e94\u6570\u636e\u5199\u5165\u589e\u52a0\u5230\u8868a\nspark.sql(\n  """\n   |merge into hadoop_prod.default.a  t1\n   |using (select id,name ,age,tp from hadoop_prod.default.b) t2\n   |on t1.id = t2.id\n   |when matched and t2.tp = \'delete\' then delete\n   |when matched and t2.tp = \'update\' then update set t1.name = t2.name,t1.age = t2.age\n   |when not matched then insert (id,name,age) values (t2.id,t2.name,t2.age)\n  """.stripMargin)\n\n'})}),"\n",(0,r.jsx)(n.h3,{id:"insert-overwrit",children:"INSERT OVERWRIT"}),"\n",(0,r.jsxs)(n.p,{children:["INSERT OVERWRITE\u53ef\u4ee5\u7528\u67e5\u8be2\u7ed3\u679c\u66ff\u6362\u8868\u4e2d\u7684\u6570\u636e\u3002\u8986\u76d6\u662f Iceberg \u8868\u7684\u539f\u5b50\u64cd\u4f5c\u3002",(0,r.jsx)("br",{}),"INSERT OVERWRITE \u66ff\u6362\u7684\u5206\u533a\u53d6\u51b3\u4e8e Spark \u7684\u5206\u533a\u8986\u76d6\u6a21\u5f0f\u548c\u8868\u7684\u5206\u533a\u60c5\u51b5\u3002 MERGE INTO \u53ea\u80fd\u91cd\u5199\u53d7\u5f71\u54cd\u7684\u6570\u636e\u6587\u4ef6\uff0c\u5e76\u4e14\u5177\u6709\u66f4\u5bb9\u6613\u7406\u89e3\u7684\u884c\u4e3a\uff0c\u56e0\u6b64\u5efa\u8bae\u4ee3\u66ff INSERT OVERWRITE\u3002",(0,r.jsx)("br",{}),"Spark\u9ed8\u8ba4\u7684\u8986\u76d6\u6a21\u5f0f\u662f\u9759\u6001\u7684\uff0c\u4f46\u662f\u5728\u5199\u5165Iceberg\u8868\u65f6\u5efa\u8bae\u4f7f\u7528\u52a8\u6001\u8986\u76d6\u6a21\u5f0f\u3002\u9759\u6001\u8986\u76d6\u6a21\u5f0f\u901a\u8fc7\u5c06PARTITION\u5b50\u53e5\u8f6c\u6362\u4e3a\u8fc7\u6ee4\u5668\u6765\u786e\u5b9a\u8981\u8986\u76d6\u8868\u4e2d\u7684\u54ea\u4e9b\u5206\u533a\uff0c\u4f46PARTITION\u5b50\u53e5\u53ea\u80fd\u5f15\u7528\u8868\u5217\u3002"]}),"\n",(0,r.jsx)(n.h4,{id:"\u52a8\u6001\u5206\u533a\u8986\u76d6",children:"\u52a8\u6001\u5206\u533a\u8986\u76d6"}),"\n",(0,r.jsx)(n.p,{children:"\u52a8\u6001\u8986\u76d6\u4f1a\u5168\u91cf\u5c06\u539f\u6709\u6570\u636e\u8986\u76d6\uff0c\u5e76\u5c06\u65b0\u63d2\u5165\u7684\u6570\u636e\u6839\u636eIceberg\u8868\u5206\u533a\u89c4\u5219\u81ea\u52a8\u5206\u533a\uff0c\u7c7b\u4f3cHive\u4e2d\u7684\u52a8\u6001\u5206\u533a\u3002"}),"\n",(0,r.jsx)(n.h4,{id:"\u9759\u6001\u5206\u533a\u8986\u76d6",children:"\u9759\u6001\u5206\u533a\u8986\u76d6"}),"\n",(0,r.jsx)(n.p,{children:"\u9759\u6001\u8986\u76d6\u9700\u8981\u5728\u5411Iceberg\u4e2d\u63d2\u5165\u6570\u636e\u65f6\u9700\u8981\u624b\u52a8\u6307\u5b9a\u5206\u533a\uff0c\u5982\u679c\u5f53\u524dIceberg\u8868\u5b58\u5728\u8fd9\u4e2a\u5206\u533a\uff0c\u90a3\u4e48\u53ea\u6709\u8fd9\u4e2a\u5206\u533a\u7684\u6570\u636e\u4f1a\u88ab\u8986\u76d6\uff0c\u5176\u4ed6\u5206\u533a\u6570\u636e\u4e0d\u53d7\u5f71\u54cd\uff0c\u5982\u679cIceberg\u8868\u4e0d\u5b58\u5728\u8fd9\u4e2a\u5206\u533a\uff0c\u90a3\u4e48\u76f8\u5f53\u4e8e\u7ed9Iceberg\u8868\u589e\u52a0\u4e86\u4e2a\u4e00\u4e2a\u5206\u533a."}),"\n",(0,r.jsx)(n.h3,{id:"update",children:"Update"}),"\n",(0,r.jsx)(n.p,{children:"Spark3.x+\u7248\u672c\u652f\u6301\u4e86update\u66f4\u65b0\u6570\u636e\u64cd\u4f5c\uff0c\u53ef\u4ee5\u6839\u636e\u5339\u914d\u7684\u6761\u4ef6\u8fdb\u884c\u6570\u636e\u66f4\u65b0\u64cd\u4f5c\u3002\u64cd\u4f5c\u5982\u4e0b\uff1a"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:'//\u521b\u5efa\u8868 delete_tbl ,\u5e76\u52a0\u8f7d\u6570\u636e\nspark.sql(\n  """\n    |create table hadoop_prod.default.update_tbl (id int,name string,age int) using iceberg\n    |""".stripMargin)\nspark.sql(\n  """\n    |insert into hadoop_prod.default.update_tbl values (1,"zs",18),(2,"ls",19),(3,"ww",20),(4,"ml",21),(5,"tq",22),(6,"gb",23)\n  """.stripMargin)\n\n'})}),"\n",(0,r.jsx)(n.p,{children:"\u901a\u8fc7\u201cupdate\u201d\u66f4\u65b0\u8868\u4e2did\u5c0f\u4e8e\u7b49\u4e8e3\u7684\u6570\u636ename\u5217\u6539\u4e3a\u201czhangsan\u201d,age\u5217\u6539\u4e3a30\uff0c\u64cd\u4f5c\u5982\u4e0b\uff1a"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-shell",children:'//\u66f4\u65b0 delete_tbl \u8868\nspark.sql(\n  """\n    |update hadoop_prod.default.update_tbl set name = \'zhangsan\' ,age = 30\n    |where id <=3\n  """.stripMargin)\nspark.sql(\n  """\n    |select * from hadoop_prod.default.update_tbl\n  """.stripMargin).show()\n\n'})}),"\n",(0,r.jsx)(n.h3,{id:"dataframe-api-\u5199\u5165iceberg\u8868",children:"DataFrame API \u5199\u5165Iceberg\u8868"}),"\n",(0,r.jsxs)(n.p,{children:["Spark\u5411Iceberg\u4e2d\u5199\u6570\u636e\u65f6\u4e0d\u4ec5\u53ef\u4ee5\u4f7f\u7528SQL\u65b9\u5f0f\uff0c\u4e5f\u53ef\u4ee5\u4f7f\u7528DataFrame Api\u65b9\u5f0f\u64cd\u4f5cIceberg,\u5efa\u8bae\u4f7f\u7528SQL\u65b9\u5f0f\u64cd\u4f5c\u3002",(0,r.jsx)("br",{}),"DataFrame\u521b\u5efaIceberg\u8868\u5206\u4e3a\u521b\u5efa\u666e\u901a\u8868\u548c\u5206\u533a\u8868\uff0c\u521b\u5efa\u5206\u533a\u8868\u65f6\u9700\u8981\u6307\u5b9a\u5206\u533a\u5217\uff0c\u5206\u533a\u5217\u53ef\u4ee5\u662f\u591a\u4e2a\u5217\u3002\u521b\u5efa\u8868\u7684\u8bed\u6cd5\u5982\u4e0b:"]}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-scala",children:"df.write(tbl).create() \u76f8\u5f53\u4e8e CREATE TABLE AS SELECT ...\ndf.write(tbl).replace() \u76f8\u5f53\u4e8e REPLACE TABLE AS SELECT ...\ndf.write(tbl).append() \u76f8\u5f53\u4e8e INSERT INTO ...\ndf.write(tbl).overwritePartitions() \u76f8\u5f53\u4e8e\u52a8\u6001 INSERT OVERWRITE ...\n"})}),"\n",(0,r.jsx)(n.p,{children:"\u5177\u4f53\u64cd\u4f5c\u5982\u4e0b"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-scala",children:'//1.\u51c6\u5907\u6570\u636e\uff0c\u4f7f\u7528DataFrame Api \u5199\u5165Iceberg\u8868\u53ca\u5206\u533a\u8868\nval nameJsonList = List[String](\n  "{\\"id\\":1,\\"name\\":\\"zs\\",\\"age\\":18,\\"loc\\":\\"beijing\\"}",\n  "{\\"id\\":2,\\"name\\":\\"ls\\",\\"age\\":19,\\"loc\\":\\"shanghai\\"}",\n  "{\\"id\\":3,\\"name\\":\\"ww\\",\\"age\\":20,\\"loc\\":\\"beijing\\"}",\n  "{\\"id\\":4,\\"name\\":\\"ml\\",\\"age\\":21,\\"loc\\":\\"shanghai\\"}")\n\nimport spark.implicits._\nval df: DataFrame = spark.read.json(nameJsonList.toDS)\n\n//\u521b\u5efa\u666e\u901a\u8868df_tbl1,\u5e76\u5c06\u6570\u636e\u5199\u5165\u5230Iceberg\u8868\uff0c\u5176\u4e2dDF\u4e2d\u7684\u5217\u5c31\u662fIceberg\u8868\u4e2d\u7684\u5217\ndf.writeTo("hadoop_prod.default.df_tbl1").create()\n\n//\u67e5\u8be2\u8868 hadoop_prod.default.df_tbl1 \u4e2d\u7684\u6570\u636e\uff0c\u5e76\u67e5\u770b\u6570\u636e\u5b58\u50a8\u7ed3\u6784\nspark.read.table("hadoop_prod.default.df_tbl1").show()\n\n'})}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-scala",children:'//\u521b\u5efa\u5206\u533a\u8868df_tbl2,\u5e76\u5c06\u6570\u636e\u5199\u5165\u5230Iceberg\u8868\uff0c\u5176\u4e2dDF\u4e2d\u7684\u5217\u5c31\u662fIceberg\u8868\u4e2d\u7684\u5217\ndf.sortWithinPartitions($"loc")//\u5199\u5165\u5206\u533a\u8868\uff0c\u5fc5\u987b\u6309\u7167\u5206\u533a\u5217\u8fdb\u884c\u6392\u5e8f\n  .writeTo("hadoop_prod.default.df_tbl2")\n  .partitionedBy($"loc")//\u8fd9\u91cc\u53ef\u4ee5\u6307\u5b9a\u591a\u4e2a\u5217\u4e3a\u8054\u5408\u5206\u533a\n  .create()\n//\u67e5\u8be2\u5206\u533a\u8868 hadoop_prod.default.df_tbl2 \u4e2d\u7684\u6570\u636e\uff0c\u5e76\u67e5\u770b\u6570\u636e\u5b58\u50a8\u7ed3\u6784\nspark.read.table("hadoop_prod.default.df_tbl2").show()\n\n'})}),"\n",(0,r.jsx)(n.h2,{id:"structured-streaming\u5b9e\u65f6\u5199\u5165iceberg",children:"Structured Streaming\u5b9e\u65f6\u5199\u5165Iceberg"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-scala",children:'import java.util.concurrent.TimeUnit\n\nimport org.apache.spark.sql.SparkSession\nimport org.apache.spark.sql.streaming.{Trigger, StreamingQuery}\nimport org.apache.spark.sql.{DataFrame, SparkSession}\n\n/**\n  * Created by Administrator on 2024/1/29 0029.\n  */\nobject StructuredStreamingSinkIceberg {\n\n  def main(args: Array[String]) {\n    System.setProperty("hadoop.home.dir", "D:\\\\aaa\\\\hadoop-2.7.6\\\\hadoop-2.7.6");\n    //\u5fc5\u987b\u8981\u8bbe\u7f6e,\u5426\u5219spark\u4f1a\u5199hive\u4f1a\u62a5HDFS\u6743\u9650\u95ee\u9898\n    System.setProperty("HADOOP_USER_NAME", "root");\n\n    val spark: SparkSession = SparkSession.builder().master("local[1]")\n    .appName("SparkIcebergDemo")\n    .config("spark.sql.catalog.hadoop_prod", "org.apache.iceberg.spark.SparkCatalog")\n    .config("spark.sql.extensions", "org.apache.iceberg.spark.extensions.IcebergSparkSessionExtensions")\n    .config("spark.sql.catalog.hadoop_prod.type", "hadoop")\n    .config("spark.sql.catalog.hadoop_prod.warehouse", "hdfs://hadoop1:9098/warehouse/sparkiceberg")\n    .getOrCreate()\n\n    spark.sparkContext.setLogLevel("WARN")\n\n    import spark.implicits._\n    //2.\u521b\u5efaIceberg \u8868\n    spark.sql(\n      """\n              |create table if not exists hadoop_prod.iceberg_db.iceberg_table (\n              | current_day string,\n              | user_id string,\n              | page_id string,\n              | channel string,\n              | action string\n              |) using iceberg\n            """.stripMargin)\n\n    //\u591a\u4e2atopic \u9017\u53f7\u5206\u5f00\n    val topic = "kafka-iceberg-topic"\n    val bootstrapServers = "hadoop1:9092,hadoop2:9092,hadoop3:9092"\n    //3.\u8bfb\u53d6Kafka\u8bfb\u53d6\u6570\u636e\n    val df = spark.readStream\n    .format("kafka")\n    .option("kafka.bootstrap.servers", bootstrapServers)\n    .option("auto.offset.reset", "earliest")\n    .option("group.id", "iceberg-kafka")\n    .option("subscribe", topic)\n    .load()\n    import spark.implicits._\n    import org.apache.spark.sql.functions._\n\n    val resDF = df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)")\n    .as[(String, String)].toDF("id", "data")\n    val transDF: DataFrame = resDF.withColumn("current_day", split(col("data"), "\\t")(0))\n    .withColumn("ts", split(col("data"), "\\t")(1))\n    .withColumn("user_id", split(col("data"), "\\t")(2))\n    .withColumn("page_id", split(col("data"), "\\t")(3))\n    .withColumn("channel", split(col("data"), "\\t")(4))\n    .withColumn("action", split(col("data"), "\\t")(5))\n    .select("current_day", "user_id", "page_id", "channel", "action")\n\n    //\u7ed3\u679c\u6253\u5370\u5230\u63a7\u5236\u53f0,Default trigger (runs micro-batch as soon as it can)\n    /*     val query: StreamingQuery = transDF.writeStream\n          .outputMode("append")\n          .format("console")\n          .option("checkpointLocation", "/tmp/checkpoint1/wordocunt8") // \u8bbe\u7f6e checkpoint \u76ee\u5f55\u4ee5\u652f\u6301 Exactly-Once \u8bed\u4e49\n          .trigger(Trigger.ProcessingTime("2 second")) // \u53ef\u4ee5\u8bbe\u7f6e\u89e6\u53d1\u5668\uff0c\u8fd9\u91cc\u6bcf\u79d2\u89e6\u53d1\u4e00\u6b21\n          .start()*/\n    val query = transDF.writeStream\n    .format("iceberg")\n    .outputMode("append")\n    //\u6bcf\u5206\u949f\u89e6\u53d1\u4e00\u6b21Trigger.ProcessingTime(1, TimeUnit.MINUTES)\n    //\u6bcf10s \u89e6\u53d1\u4e00\u6b21 Trigger.ProcessingTime(1, TimeUnit.MINUTES)\n    .trigger(Trigger.ProcessingTime(10, TimeUnit.SECONDS))\n    .option("path", "hadoop_prod.iceberg_db.iceberg_table")\n    .option("fanout-enabled", "true")\n    .option("checkpointLocation", "/tmp/checkpoint1/wordocunt8") // \u8bbe\u7f6e checkpoint \u76ee\u5f55\u4ee5\u652f\u6301 Exactly-Once \u8bed\u4e49\n    .start()\n\n    query.awaitTermination()\n  }\n\n}\n\n'})}),"\n",(0,r.jsx)(n.h2,{id:"iceberg\u96c6\u6210flink",children:"Iceberg\u96c6\u6210Flink"}),"\n",(0,r.jsxs)(n.blockquote,{children:["\n",(0,r.jsx)(n.p,{children:"Flink \u7248\u672c1.16.2, Iceberg \u7248\u672c1.4.3"}),"\n"]}),"\n",(0,r.jsx)(n.h3,{id:"maven-pom",children:"Maven Pom"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-xml",children:'<?xml version="1.0" encoding="UTF-8"?>\n<project xmlns="http://maven.apache.org/POM/4.0.0"\n  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"\n  xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">\n  <modelVersion>4.0.0</modelVersion>\n\n  <groupId>com.daiyutage</groupId>\n  <artifactId>Flink-Iceberg</artifactId>\n  <version>1.0-SNAPSHOT</version>\n\n  <properties>\n    <maven.compiler.source>8</maven.compiler.source>\n    <maven.compiler.target>8</maven.compiler.target>\n    <project.build.sourceEncoding>UTF-8</project.build.sourceEncoding>\n    <flink-version>1.16.2</flink-version>\n  </properties>\n  <dependencies>\n    <dependency>\n      <groupId>org.apache.flink</groupId>\n      <artifactId>flink-core</artifactId>\n      <version>${flink-version}</version>\n    </dependency>\n    <dependency>\n      <groupId>org.apache.flink</groupId>\n      <artifactId>flink-clients</artifactId>\n      <version>${flink-version}</version>\n    </dependency>\n    <dependency>\n      <groupId>org.apache.flink</groupId>\n      <artifactId>flink-streaming-java</artifactId>\n      <version>${flink-version}</version>\n    </dependency>\n    <dependency>\n      <groupId>org.apache.flink</groupId>\n      <artifactId>flink-table-planner_2.12</artifactId>\n      <version>${flink-version}</version>\n    </dependency>\n    <dependency>\n      <groupId>org.apache.flink</groupId>\n      <artifactId>flink-connector-kafka</artifactId>\n      <version>${flink-version}</version>\n    </dependency>\n    <dependency>\n      <groupId>org.apache.flink</groupId>\n      <artifactId>flink-runtime-web</artifactId>\n      <version>${flink-version}</version>\n    </dependency>\n\n\n\n    \x3c!--        <dependency>\n    <groupId>org.apache.iceberg</groupId>\n    <artifactId>iceberg-flink</artifactId>\n    <version>1.4.3</version>\n  </dependency>--\x3e\n    <dependency>\n      <groupId>org.apache.iceberg</groupId>\n      <artifactId>iceberg-flink-runtime-1.16</artifactId>\n      <version>1.4.3</version>\n    </dependency>\n    <dependency>\n      <groupId>org.apache.hadoop</groupId>\n      <artifactId>hadoop-common</artifactId>\n      \x3c!--<version>2.7.2</version>--\x3e\n      <version>3.2.2</version>\n    </dependency>\n    <dependency>\n      <groupId>org.apache.hadoop</groupId>\n      <artifactId>hadoop-client</artifactId>\n      <version>3.2.0</version>\n    </dependency>\n  </dependencies>\n</project>\n'})}),"\n",(0,r.jsx)(n.h3,{id:"\u521b\u5efa\u8868-1",children:"\u521b\u5efa\u8868"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:'package com.daiyutage.flink.iceberg;\n\nimport org.apache.flink.api.common.functions.MapFunction;\nimport org.apache.flink.streaming.api.datastream.DataStream;\nimport org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\nimport org.apache.flink.table.api.EnvironmentSettings;\nimport org.apache.flink.table.api.TableEnvironment;\nimport org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\nimport org.apache.flink.table.data.RowData;\nimport org.apache.flink.table.data.StringData;\nimport org.apache.iceberg.flink.TableLoader;\nimport org.apache.iceberg.flink.source.FlinkSource;\n\n/**\n * Created by Administrator on 2024/1/31 0031.\n */\npublic class FlinkIcebergExample2 {\n\n    public static void main(String[] args) throws Exception {\n\n        /**\n         * \u4f7f\u7528Flink SQL \u521b\u5efaIceberg\u8868,\u5e76\u5199\u5165\u6570\u636e\n         */\n\n        System.setProperty("hadoop.home.dir", "D:\\\\aaa\\\\hadoop-2.7.6\\\\hadoop-2.7.6");\n        //\u5fc5\u987b\u8981\u8bbe\u7f6e,\u5426\u5219spark\u4f1a\u5199hive\u4f1a\u62a5HDFS\u6743\u9650\u95ee\u9898\n        System.setProperty("HADOOP_USER_NAME", "root");\n\n        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n        StreamTableEnvironment tblEnv = StreamTableEnvironment.create(env);\n\n        env.enableCheckpointing(1000);\n\n        //1.\u521b\u5efaCatalog\n        tblEnv.executeSql("CREATE CATALOG hadoop_iceberg WITH (" +\n                          "\'type\'=\'iceberg\'," +\n                          "\'catalog-type\'=\'hadoop\'," +\n                          "\'warehouse\'=\'hdfs://hadoop1:9098/warehouse/flinkiceberg\')");\n\n        //2.\u4f7f\u7528\u5f53\u524dCatalog\n        tblEnv.useCatalog("hadoop_iceberg");\n\n        //3.\u521b\u5efa\u6570\u636e\u5e93\n        tblEnv.executeSql("use database iceberg_db");\n\n        //4.\u4f7f\u7528\u6570\u636e\u5e93\n        tblEnv.useDatabase("iceberg_db");\n\n        //5.\u521b\u5efaiceberg\u8868 flink_iceberg_tbl\n        tblEnv.executeSql("create table hadoop_iceberg.iceberg_db.flink_iceberg_tbl1(id int,name string,age int,loc string) partitioned by (loc)");\n\n        //6.\u5199\u5165\u6570\u636e\u5230\u8868 flink_iceberg_tbl\n               tblEnv.executeSql("insert into hadoop_iceberg.iceberg_db.flink_iceberg_tbl1 values (1,\'zs\',18,\'beijing\'),(2,\'ls\',19,\'shanghai\'),(3,\'ww\',20,\'guangzhou\')");\n\n    }\n}\n\n'})}),"\n",(0,r.jsx)(n.h3,{id:"\u8bfb\u53d6\u8868",children:"\u8bfb\u53d6\u8868"}),"\n",(0,r.jsx)(n.p,{children:"Flink\u53ef\u4ee5\u6279\u91cf\u8bfb\u53d6\u8868\u6216\u8005\u662f\u6d41\u5f0f\u8bfb\u53d6\u8868\u3002"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:'package com.daiyutage.flink.iceberg;\n\nimport org.apache.flink.api.common.functions.MapFunction;\nimport org.apache.flink.streaming.api.datastream.DataStream;\nimport org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\nimport org.apache.flink.table.data.RowData;\nimport org.apache.flink.table.data.StringData;\nimport org.apache.iceberg.flink.TableLoader;\nimport org.apache.iceberg.flink.source.FlinkSource;\n\n/**\n * Created by Administrator on 2024/1/31 0031.\n */\npublic class FlinkIcebergExample1 {\n\n    public static void main(String[] args) throws Exception {\n\n        /**\n         * \u4f7f\u7528Flink DataStream API \u6279\u91cf(\u6d41\u5931)\u8bfb\u53d6Iceberg\u8868\u6570\u636e\n         */\n\n        System.setProperty("hadoop.home.dir", "D:\\\\aaa\\\\hadoop-2.7.6\\\\hadoop-2.7.6");\n        //\u5fc5\u987b\u8981\u8bbe\u7f6e,\u5426\u5219spark\u4f1a\u5199hive\u4f1a\u62a5HDFS\u6743\u9650\u95ee\u9898\n        System.setProperty("HADOOP_USER_NAME", "root");\n        StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironment();\n        TableLoader tableLoader = TableLoader.fromHadoopTable("hdfs://hadoop1:9098/warehouse/flinkiceberg/iceberg_db/flink_iceberg_tbl2");\n        DataStream<org.apache.flink.table.data.RowData> dataStream = FlinkSource.forRowData()\n        .env(env)\n        .tableLoader(tableLoader)\n        .streaming(true) //true\u4e3a\u6d41\u5f0f\u8bfb\u53d6,false\u4e3a\u6279\u91cf\u4e00\u6b21\u8bfb\u53d6\n        .build();\n\n        /**\n         *\n         *         val transDF: DataFrame = resDF.withColumn("current_day", split(col("data"), "\\t")(0))\n         .withColumn("ts", split(col("data"), "\\t")(1))\n         .withColumn("user_id", split(col("data"), "\\t")(2))\n         .withColumn("page_id", split(col("data"), "\\t")(3))\n         .withColumn("channel", split(col("data"), "\\t")(4))\n         .withColumn("action", split(col("data"), "\\t")(5))\n         .select("current_day", "user_id", "page_id", "channel", "action")\n         */\n        // Print all records to stdout.\n        dataStream.map(new MapFunction<RowData, String>() {\n            @Override\n            public String map(RowData rowData) throws Exception {\n                Integer current_day = rowData.getInt(0);\n                StringData user_id = rowData.getString(1);\n                Integer age = rowData.getInt(2);\n                StringData channel = rowData.getString(3);\n                return current_day + "," + user_id + "," + age + "," + channel ;\n            }\n        }).print();\n\n        // Submit and execute this batch read job.\n        env.execute("Test Iceberg Batch Read");\n    }\n}\n'})}),"\n",(0,r.jsx)(n.h3,{id:"\u5b9e\u65f6\u5199\u5165\u8868",children:"\u5b9e\u65f6\u5199\u5165\u8868"}),"\n",(0,r.jsx)(n.p,{children:"\u6d88\u8d39Kafka\u6570\u636e\uff0c\u7136\u540e\u5b9e\u65f6\u5199\u5165Iceberg\u8868\u3002\u6ce8\u610f\uff0c\u5fc5\u987b\u8981\u5f00\u542fcheckpoint\u673a\u5236\uff0c\u5426\u5219\u4e0d\u4f1acommit\u6570\u636e,\u5199\u5165\u4e4b\u540e\u662f\u6ca1\u6709\u6548\u679c\u7684\u3002"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:'package com.daiyutage.flink.iceberg;\n\nimport com.google.common.collect.ImmutableMap;\nimport org.apache.flink.api.common.eventtime.WatermarkStrategy;\nimport org.apache.flink.api.common.functions.MapFunction;\nimport org.apache.flink.api.common.serialization.SimpleStringSchema;\nimport org.apache.flink.configuration.Configuration;\nimport org.apache.flink.configuration.RestOptions;\nimport org.apache.flink.connector.kafka.source.KafkaSource;\nimport org.apache.flink.connector.kafka.source.enumerator.initializer.OffsetsInitializer;\nimport org.apache.flink.streaming.api.datastream.DataStreamSource;\nimport org.apache.flink.streaming.api.datastream.SingleOutputStreamOperator;\nimport org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\nimport org.apache.flink.streaming.api.functions.sink.PrintSinkFunction;\nimport org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\nimport org.apache.flink.table.data.GenericRowData;\nimport org.apache.flink.table.data.RowData;\nimport org.apache.flink.table.data.StringData;\nimport org.apache.iceberg.*;\nimport org.apache.iceberg.catalog.TableIdentifier;\nimport org.apache.iceberg.flink.TableLoader;\nimport org.apache.iceberg.flink.sink.FlinkSink;\nimport org.apache.iceberg.hadoop.HadoopCatalog;\nimport org.apache.iceberg.types.Types;\n\nimport java.util.Map;\n\n/**\n * Created by Administrator on 2024/2/3 0003.\n */\npublic class KafkaToIceberg {\n\n    public static void main(String[] args) throws Exception {\n\n\n        System.setProperty("hadoop.home.dir", "D:\\\\aaa\\\\hadoop-2.7.6\\\\hadoop-2.7.6");\n        //\u5fc5\u987b\u8981\u8bbe\u7f6e,\u5426\u5219spark\u4f1a\u5199hive\u4f1a\u62a5HDFS\u6743\u9650\u95ee\u9898\n        System.setProperty("HADOOP_USER_NAME", "root");\n\n        Configuration conf = new Configuration();\n        //\u8bbe\u7f6eWebUI\u7ed1\u5b9a\u7684\u672c\u5730\u7aef\u53e3\n        conf.setString(RestOptions.BIND_PORT, "8086");\n\n        //        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment(conf);\n        StreamExecutionEnvironment env = StreamExecutionEnvironment.createLocalEnvironmentWithWebUI(conf);\n        //        StreamTableEnvironment tblEnv = StreamTableEnvironment.create(env);\n        env.enableCheckpointing(2000);\n        env.getCheckpointConfig().setCheckpointStorage("file:///D:\\\\Program\\\\Flink-Project\\\\Flink-Iceberg\\\\checkpoint");\n        env.disableOperatorChaining();\n\n        String bootStrapServer = "hadoop1:9092,hadoop2:9092,hadoop3:9092";\n        KafkaSource<String> kafkaSource = KafkaSource.<String>builder()\n        .setBootstrapServers(bootStrapServer)\n        .setTopics("mytopic")\n        .setGroupId("my-group-id-1")\n        .setStartingOffsets(OffsetsInitializer.latest())\n        .setValueOnlyDeserializer(new SimpleStringSchema())\n        .build();\n        DataStreamSource<String> kafkaDS = env.fromSource(kafkaSource, WatermarkStrategy.noWatermarks(), "kafkaSource");\n\n        SingleOutputStreamOperator<RowData> mapDS = kafkaDS.map(new MapFunction<String, RowData>() {\n            @Override\n            public RowData map(String line) throws Exception {\n                String[] split = line.split(",");\n                GenericRowData row = new GenericRowData(4);\n                row.setField(0, Integer.valueOf(split[0]));\n                row.setField(1, StringData.fromString(split[1]));\n                row.setField(2, Integer.valueOf(split[2]));\n                row.setField(3, StringData.fromString(split[3]));\n                return row;\n            }\n        });\n\n        org.apache.hadoop.conf.Configuration config = new org.apache.hadoop.conf.Configuration();\n        HadoopCatalog catalog = new HadoopCatalog(config, "hdfs://hadoop1:9098/warehouse/flinkiceberg");\n        //\u914d\u7f6eiceberg \u5e93\u540d\u548c\u8868\u540d\n        TableIdentifier name = TableIdentifier.of("icebergdb", "flink_iceberg_tbl1");\n        //\u521b\u5efaIcebeng\u8868Schema\n        Schema schema = new Schema(\n            Types.NestedField.required(1, "id", Types.IntegerType.get()),\n            Types.NestedField.required(2, "nane", Types.StringType.get()),\n            Types.NestedField.required(3, "age", Types.IntegerType.get()),\n            Types.NestedField.required(4, "loc", Types.StringType.get()));\n        //\u5982\u679c\u6709\u5206\u533a\u6307\u5b9a\u5bf9\u5e94\u5206\u533a\uff0c\u8fd9\u91cc\u201cloc\u201d\u5217\u4e3a\u5206\u533a\u5217\uff0c\u53ef\u4ee5\u6307\u5b9aunpartitioned \u65b9\u6cd5\u4e0d\u8bbe\u7f6e\u8868\u5206\u533a\n        //        PartitionSpec spec = PartitionSpec.unpartitioned();\n        PartitionSpec spec = PartitionSpec.builderFor(schema).identity("loc").build();\n        Map<String, String> props =\n        ImmutableMap.of(TableProperties.DEFAULT_FILE_FORMAT, FileFormat.PARQUET.name());\n        Table table = null;\n\n        // \u901a\u8fc7catalog\u5224\u65ad\u8868\u662f\u5426\u5b58\u5728\uff0c\u4e0d\u5b58\u5728\u5c31\u521b\u5efa\uff0c\u5b58\u5728\u5c31\u52a0\u8f7d\n        if (!catalog.tableExists(name)) {\n            table = catalog.createTable(name, schema, spec, props);\n        }else {\n            table = catalog.loadTable(name);\n        }\n\n        mapDS.addSink(new PrintSinkFunction<>());\n        TableLoader tableLoader = TableLoader.fromHadoopTable("hdfs://hadoop1:9098/warehouse/flinkiceberg/iceberg_db/flink_iceberg_tbl1", config);\n\n        //5.\u901a\u8fc7DataStream Api \u5411Iceberg\u4e2d\u5199\u5165\u6570\u636e\n        FlinkSink.forRowData(mapDS)\n        //\u8fd9\u4e2a .table \u4e5f\u53ef\u4ee5\u4e0d\u5199\uff0c\u6307\u5b9atableLoader \u5bf9\u5e94\u7684\u8def\u5f84\u5c31\u53ef\u4ee5\u3002\n        //                .table(table)\n        .tableLoader(tableLoader)\n        //\u9ed8\u8ba4\u4e3afalse,\u8ffd\u52a0\u6570\u636e\u3002\u5982\u679c\u8bbe\u7f6e\u4e3atrue \u5c31\u662f\u8986\u76d6\u6570\u636e\n        .overwrite(false)\n        .append();\n\n        kafkaDS.name("KafkaSource").addSink(new PrintSinkFunction<>());\n\n        env.execute();\n\n\n    }\n}\n\n'})}),"\n",(0,r.jsx)(n.p,{children:"Kafka\u6837\u4f8b\u6570\u636e"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:"[root@node1 bin]#bin//kafka-console-producer.sh  --topic mytopic --broker-list hadoop1:9092,hadoop2:9092,hadoop3:9092\n1,zs,18,beijing\n2,ls,19,shanghai\n3,ww,20,beijing\n4,ml,21,shanghai\n"})}),"\n",(0,r.jsx)(n.h3,{id:"sql-api-\u64cd\u4f5c",children:"SQL API \u64cd\u4f5c"}),"\n",(0,r.jsx)(n.h4,{id:"\u6279\u91cf\u8bfb\u53d6\u8868",children:"\u6279\u91cf\u8bfb\u53d6\u8868"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:'System.setProperty("hadoop.home.dir", "D:\\\\aaa\\\\hadoop-2.7.6\\\\hadoop-2.7.6");\n//\u5fc5\u987b\u8981\u8bbe\u7f6e,\u5426\u5219spark\u4f1a\u5199hive\u4f1a\u62a5HDFS\u6743\u9650\u95ee\u9898\nSystem.setProperty("HADOOP_USER_NAME", "root");\n\nStreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\nStreamTableEnvironment tblEnv = StreamTableEnvironment.create(env);\n\nenv.enableCheckpointing(1000);\n\n//1.\u521b\u5efaCatalog\ntblEnv.executeSql("CREATE CATALOG hadoop_iceberg WITH (" +\n                  "\'type\'=\'iceberg\'," +\n                  "\'catalog-type\'=\'hadoop\'," +\n                  "\'warehouse\'=\'hdfs://hadoop1:9098/warehouse/flinkiceberg\')");\n\n\nTableResult tableResult = tblEnv.executeSql("select * from hadoop_iceberg.iceberg_db.flink_iceberg_tbl2");\n\ntableResult.print();;\n'})}),"\n",(0,r.jsx)(n.h4,{id:"\u5b9e\u65f6\u8bfb\u53d6\u8868",children:"\u5b9e\u65f6\u8bfb\u53d6\u8868"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:"package com.daiyutage.flink.iceberg;\n\nimport org.apache.flink.configuration.Configuration;\nimport org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\nimport org.apache.flink.table.api.TableResult;\nimport org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n\n/**\n * Created by Administrator on 2024/2/3 0003.\n */\npublic class FlinkSqlIceberg2 {\n\n    public static void main(String[] args) {\n\n        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n        StreamTableEnvironment tblEnv = StreamTableEnvironment.create(env);\n\n        env.enableCheckpointing(1000);\n\n        Configuration configuration = tblEnv.getConfig().getConfiguration();\n        // \u652f\u6301SQL\u8bed\u6cd5\u4e2d\u7684 OPTIONS \u9009\u9879\n        configuration.setBoolean(\"table.dynamic-table-options.enabled\", true);\n\n        //1.\u521b\u5efaCatalog\n        tblEnv.executeSql(\"CREATE CATALOG hadoop_iceberg WITH (\" +\n                          \"'type'='iceberg',\" +\n                          \"'catalog-type'='hadoop',\" +\n                          \"'warehouse'='hdfs://hadoop1:9098/warehouse/flinkiceberg')\");\n\n        //2.\u4eceIceberg\u8868\u5f53\u524d\u5feb\u7167\u8bfb\u53d6\u6240\u6709\u6570\u636e\uff0c\u5e76\u7ee7\u7eed\u589e\u91cf\u8bfb\u53d6\u6570\u636e\n        // streaming\u6307\u5b9a\u4e3atrue\u652f\u6301\u5b9e\u65f6\u8bfb\u53d6\u6570\u636e\uff0cmonitor_interval \u76d1\u63a7\u6570\u636e\u7684\u95f4\u9694\uff0c\u9ed8\u8ba41s\n        TableResult tableResult = tblEnv.executeSql(\"select * from hadoop_iceberg.iceberg_db.flink_iceberg_tbl2 /*+ OPTIONS('streaming'='true', 'monitor-interval'='1s')*/\");\n        tableResult.print();\n        ;\n    }\n}\n\n"})}),"\n",(0,r.jsx)(n.h4,{id:"\u8bfb\u53d6kafka\u5199\u5165\u8868",children:"\u8bfb\u53d6Kafka\u5199\u5165\u8868"}),"\n",(0,r.jsx)(n.p,{children:"\u5b9e\u65f6\u6d88\u8d39Kafka\u6570\u636e\uff0c\u7136\u540e\u5199\u5165Iceberg\u8868"}),"\n",(0,r.jsx)(n.pre,{children:(0,r.jsx)(n.code,{className:"language-java",children:'package com.daiyutage.flink.iceberg;\n\nimport org.apache.flink.configuration.Configuration;\nimport org.apache.flink.streaming.api.environment.StreamExecutionEnvironment;\nimport org.apache.flink.table.api.TableResult;\nimport org.apache.flink.table.api.bridge.java.StreamTableEnvironment;\n\n/**\n * Created by Administrator on 2024/2/3 0003.\n */\npublic class FlinkSqlToIceberg {\n\n    public static void main(String[] args) {\n        /**\n         * \u5b9e\u65f6\u6d88\u8d39kafka\u6570\u636e\u5199\u5165Iceberg\u8868\n         */\n\n        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n        StreamTableEnvironment tblEnv = StreamTableEnvironment.create(env);\n\n        env.enableCheckpointing(5000);\n\n        Configuration configuration = tblEnv.getConfig().getConfiguration();\n        // \u652f\u6301SQL\u8bed\u6cd5\u4e2d\u7684 OPTIONS \u9009\u9879\n        configuration.setBoolean("table.dynamic-table-options.enabled", true);\n\n        //1.\u521b\u5efaCatalog\n        tblEnv.executeSql("CREATE CATALOG hadoop_iceberg WITH (" +\n                          "\'type\'=\'iceberg\'," +\n                          "\'catalog-type\'=\'hadoop\'," +\n                          "\'warehouse\'=\'hdfs://hadoop1:9098/warehouse/flinkiceberg\')");\n        //3.\u521b\u5efa Kafka Connector,\u8fde\u63a5\u6d88\u8d39Kafka\u4e2d\u6570\u636e\n        tblEnv.executeSql("create table kafka_input_table(" +\n                          " id int," +\n                          " name varchar," +\n                          " age int," +\n                          " loc varchar" +\n                          ") with (" +\n                          " \'connector\' = \'kafka\'," +\n                          " \'topic\' = \'mytopic\'," +\n                          " \'properties.bootstrap.servers\'=\'hadoop1:9092,hadoop2:9092,hadoop3:9092\'," +\n                          " \'scan.startup.mode\'=\'latest-offset\'," +\n                          " \'properties.group.id\' = \'my-group-id\'," +\n                          " \'format\' = \'csv\'" +\n                          ")");\n\n        TableResult tableResult = tblEnv.executeSql("select * from kafka_input_table");\n        tableResult.print();;\n\n    }\n}\n\n'})})]})}function p(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,r.jsx)(n,{...e,children:(0,r.jsx)(d,{...e})}):d(e)}},8453:(e,n,a)=>{a.d(n,{R:()=>o,x:()=>s});var r=a(6540);const t={},i=r.createContext(t);function o(e){const n=r.useContext(i);return r.useMemo((function(){return"function"==typeof e?e(n):{...n,...e}}),[n,e])}function s(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:o(e.components),r.createElement(i.Provider,{value:n},e.children)}}}]);