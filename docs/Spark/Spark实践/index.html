<!doctype html>
<html lang="en" dir="ltr" class="docs-wrapper plugin-docs plugin-id-default docs-version-current docs-doc-page docs-doc-id-Spark/Spark实践" data-has-hydrated="false">
<head>
<meta charset="UTF-8">
<meta name="generator" content="Docusaurus v3.1.1">
<title data-rh="true">Spark实践 | 大数据Guide</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:image" content="https://your-docusaurus-site.example.com/img/docusaurus-social-card.jpg"><meta data-rh="true" name="twitter:image" content="https://your-docusaurus-site.example.com/img/docusaurus-social-card.jpg"><meta data-rh="true" property="og:url" content="https://your-docusaurus-site.example.com/docs/Spark/Spark实践"><meta data-rh="true" property="og:locale" content="en"><meta data-rh="true" name="docusaurus_locale" content="en"><meta data-rh="true" name="docsearch:language" content="en"><meta data-rh="true" name="docusaurus_version" content="current"><meta data-rh="true" name="docusaurus_tag" content="docs-default-current"><meta data-rh="true" name="docsearch:version" content="current"><meta data-rh="true" name="docsearch:docusaurus_tag" content="docs-default-current"><meta data-rh="true" property="og:title" content="Spark实践 | 大数据Guide"><meta data-rh="true" name="description" content="什么是RDD2"><meta data-rh="true" property="og:description" content="什么是RDD2"><link data-rh="true" rel="icon" href="/img/favicon.ico"><link data-rh="true" rel="canonical" href="https://your-docusaurus-site.example.com/docs/Spark/Spark实践"><link data-rh="true" rel="alternate" href="https://your-docusaurus-site.example.com/docs/Spark/Spark实践" hreflang="en"><link data-rh="true" rel="alternate" href="https://your-docusaurus-site.example.com/docs/Spark/Spark实践" hreflang="x-default"><link rel="alternate" type="application/rss+xml" href="/blog/rss.xml" title="大数据Guide RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/blog/atom.xml" title="大数据Guide Atom Feed"><link rel="stylesheet" href="/assets/css/styles.69f2a557.css">
<script src="/assets/js/runtime~main.9ccc24a5.js" defer="defer"></script>
<script src="/assets/js/main.901e4aca.js" defer="defer"></script>
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){try{return new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}}()||function(){try{return localStorage.getItem("theme")}catch(t){}}();t(null!==e?e:"light")}(),function(){try{const c=new URLSearchParams(window.location.search).entries();for(var[t,e]of c)if(t.startsWith("docusaurus-data-")){var a=t.replace("docusaurus-data-","data-");document.documentElement.setAttribute(a,e)}}catch(t){}}()</script><div id="__docusaurus"><div role="region" aria-label="Skip to main content"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">Skip to main content</a></div><nav aria-label="Main" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="Toggle navigation bar" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/"><div class="navbar__logo"><img src="/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--light_NVdE"><img src="/img/logo.svg" alt="My Site Logo" class="themedComponent_mlkZ themedComponent--dark_xIcU"></div><b class="navbar__title text--truncate">大数据Guide</b></a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/docs/Flink/Flink SQL JOIN原理">大数据</a></div><div class="navbar__items navbar__items--right"><div class="toggle_vylO colorModeToggle_DEke"><button class="clean-btn toggleButton_gllP toggleButtonDisabled_aARS" type="button" disabled="" title="Switch between dark and light mode (currently light mode)" aria-label="Switch between dark and light mode (currently light mode)" aria-live="polite"><svg viewBox="0 0 24 24" width="24" height="24" class="lightToggleIcon_pyhR"><path fill="currentColor" d="M12,9c1.65,0,3,1.35,3,3s-1.35,3-3,3s-3-1.35-3-3S10.35,9,12,9 M12,7c-2.76,0-5,2.24-5,5s2.24,5,5,5s5-2.24,5-5 S14.76,7,12,7L12,7z M2,13l2,0c0.55,0,1-0.45,1-1s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S1.45,13,2,13z M20,13l2,0c0.55,0,1-0.45,1-1 s-0.45-1-1-1l-2,0c-0.55,0-1,0.45-1,1S19.45,13,20,13z M11,2v2c0,0.55,0.45,1,1,1s1-0.45,1-1V2c0-0.55-0.45-1-1-1S11,1.45,11,2z M11,20v2c0,0.55,0.45,1,1,1s1-0.45,1-1v-2c0-0.55-0.45-1-1-1C11.45,19,11,19.45,11,20z M5.99,4.58c-0.39-0.39-1.03-0.39-1.41,0 c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0s0.39-1.03,0-1.41L5.99,4.58z M18.36,16.95 c-0.39-0.39-1.03-0.39-1.41,0c-0.39,0.39-0.39,1.03,0,1.41l1.06,1.06c0.39,0.39,1.03,0.39,1.41,0c0.39-0.39,0.39-1.03,0-1.41 L18.36,16.95z M19.42,5.99c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06c-0.39,0.39-0.39,1.03,0,1.41 s1.03,0.39,1.41,0L19.42,5.99z M7.05,18.36c0.39-0.39,0.39-1.03,0-1.41c-0.39-0.39-1.03-0.39-1.41,0l-1.06,1.06 c-0.39,0.39-0.39,1.03,0,1.41s1.03,0.39,1.41,0L7.05,18.36z"></path></svg><svg viewBox="0 0 24 24" width="24" height="24" class="darkToggleIcon_wfgR"><path fill="currentColor" d="M9.37,5.51C9.19,6.15,9.1,6.82,9.1,7.5c0,4.08,3.32,7.4,7.4,7.4c0.68,0,1.35-0.09,1.99-0.27C17.45,17.19,14.93,19,12,19 c-3.86,0-7-3.14-7-7C5,9.07,6.81,6.55,9.37,5.51z M12,3c-4.97,0-9,4.03-9,9s4.03,9,9,9s9-4.03,9-9c0-0.46-0.04-0.92-0.1-1.36 c-0.98,1.37-2.58,2.26-4.4,2.26c-2.98,0-5.4-2.42-5.4-5.4c0-1.81,0.89-3.42,2.26-4.4C12.92,3.04,12.46,3,12,3L12,3z"></path></svg></button></div><div class="navbarSearchContainer_Bca1"></div></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div class="docsWrapper_hBAB"><button aria-label="Scroll back to top" class="clean-btn theme-back-to-top-button backToTopButton_sjWU" type="button"></button><div class="docRoot_UBD9"><aside class="theme-doc-sidebar-container docSidebarContainer_YfHR"><div class="sidebarViewport_aRkj"><div class="sidebar_njMd"><nav aria-label="Docs sidebar" class="menu thin-scrollbar menu_SIkG"><ul class="theme-doc-sidebar-menu menu__list"><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Flink/Flink SQL JOIN原理">Flink</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Hadoop/Hadoop实践">Hadoop</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Hive/Hive调优">Hive</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Kafka/Kafka实践">Kafka</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/OLAP/Clickhouse/Clickhouse存算分离">OLAP</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret menu__link--active" aria-expanded="true" href="/docs/Spark/Spark面试题1">Spark</a></div><ul style="display:block;overflow:visible;height:auto" class="menu__list"><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Spark/Spark面试题1">Spark面试题1</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Spark/Spark面试题2">Spark面试题2</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link menu__link--active" aria-current="page" tabindex="0" href="/docs/Spark/Spark实践">Spark实践</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Spark/SparkStreamingCheckpoint机制">SparkStreamingCheckpoint机制</a></li><li class="theme-doc-sidebar-item-link theme-doc-sidebar-item-link-level-2 menu__list-item"><a class="menu__link" tabindex="0" href="/docs/Spark/Spark数据倾斜调优">Spark数据倾斜调优</a></li></ul></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/Zookeeper/Zookeeper面试题">Zookeeper</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/其他/Flume面试题">其他</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/数据仓库/数仓架构建设方法论">数据仓库</a></div></li><li class="theme-doc-sidebar-item-category theme-doc-sidebar-item-category-level-1 menu__list-item menu__list-item--collapsed"><div class="menu__list-item-collapsible"><a class="menu__link menu__link--sublist menu__link--sublist-caret" aria-expanded="false" href="/docs/数据湖/Iceberg/Iceberg实践">数据湖</a></div></li></ul></nav></div></div></aside><main class="docMainContainer_TBSr"><div class="container padding-top--md padding-bottom--lg"><div class="row"><div class="col docItemCol_VOVn"><div class="docItemContainer_Djhp"><article><nav class="theme-doc-breadcrumbs breadcrumbsContainer_Z_bl" aria-label="Breadcrumbs"><ul class="breadcrumbs" itemscope="" itemtype="https://schema.org/BreadcrumbList"><li class="breadcrumbs__item"><a aria-label="Home page" class="breadcrumbs__link" href="/"><svg viewBox="0 0 24 24" class="breadcrumbHomeIcon_YNFT"><path d="M10 19v-5h4v5c0 .55.45 1 1 1h3c.55 0 1-.45 1-1v-7h1.7c.46 0 .68-.57.33-.87L12.67 3.6c-.38-.34-.96-.34-1.34 0l-8.36 7.53c-.34.3-.13.87.33.87H5v7c0 .55.45 1 1 1h3c.55 0 1-.45 1-1z" fill="currentColor"></path></svg></a></li><li class="breadcrumbs__item"><span class="breadcrumbs__link">Spark</span><meta itemprop="position" content="1"></li><li itemscope="" itemprop="itemListElement" itemtype="https://schema.org/ListItem" class="breadcrumbs__item breadcrumbs__item--active"><span class="breadcrumbs__link" itemprop="name">Spark实践</span><meta itemprop="position" content="2"></li></ul></nav><div class="tocCollapsible_ETCw theme-doc-toc-mobile tocMobile_ITEo"><button type="button" class="clean-btn tocCollapsibleButton_TO0P">On this page</button></div><div class="theme-doc-markdown markdown"><header><h1>Spark实践</h1></header><h2 class="anchor anchorWithStickyNavbar_LWe7" id="什么是rdd2">什么是RDD2<a href="#什么是rdd2" class="hash-link" aria-label="Direct link to 什么是RDD2" title="Direct link to 什么是RDD2">​</a></h2>
<p>参考：<a href="https://cloud.tencent.com/developer/article/1780260" target="_blank" rel="noopener noreferrer">https://cloud.tencent.com/developer/article/1780260</a></p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="概念">概念<a href="#概念" class="hash-link" aria-label="Direct link to 概念" title="Direct link to 概念">​</a></h3>
<p>RDD（Resilient Distributed Dataset）叫做分布式数据集，是Spark中最基本的数据抽象。代码中是一个抽象类，它代表一个不可变、可分区、里面的元素可<a href="https://cloud.tencent.com/product/gpu?from=20065&amp;from_column=20065" target="_blank" rel="noopener noreferrer">并行计算</a>的集合。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="rdd的五个特性">RDD的五个特性<a href="#rdd的五个特性" class="hash-link" aria-label="Direct link to RDD的五个特性" title="Direct link to RDD的五个特性">​</a></h3>
<ul>
<li>1） A list of partitions<br>一个分区列表，一个rdd有多个分区，后期spark任务计算是以分区为单位，一个分区就对应上一个task线程。 通过val rdd1=sc.textFile(文件) 如果这个文件大小的block个数 小于等于2，它产生的rdd的分区数就是2 如果这个文件大小的block个数大于2，它产生的rdd的分区数跟文件的block相同。<br>- 2）A function for computing each split<br>由一个函数计算每一个分片 比如： rdd2=rdd1.map(x=&gt;(x,1)) ，这里指的就是每个单词计为1的函数<br>- 3）A list of dependencies on other RDDs<br>一个rdd会依赖于其他多个rdd，这里就涉及到rdd与rdd之间的依赖关系，后期spark任务的容错机制就是根据这个特性而来。 比如： rdd2=rdd1.map(x=&gt;(x,1)) rdd2的结果是通过rdd1调用了map方法生成，那么rdd2就依赖于rdd1的结果 对其他RDD的依赖列表，依赖还具体分为宽依赖和窄依赖，但并不是所有的RDD都有依赖。<br>- 4)Optionally, a Partitioner for key-value RDDs (e.g. to say that the RDD is hash-partitioned)<br>(可选项) 对于kv类型的rdd才会有分区函数（必须要产生shuffle），对于不是kv类型的rdd分区函数是None。 分区函数的作用：它是决定了原始rdd的数据会流入到下面rdd的哪些分区中。 spark的分区函数有2种：第一种hashPartitioner（默认值）, 通过 key.hashcode % 分区数=分区号 第二种RangePartitioner,是基于一定的范围进行分区。<br>- 5)Optionally, a list of preferred locations to compute each split on (e.g. block locations for an HDFS file)<br>(可选项) 一组最优的数据块的位置，这里涉及到数据的本地性和数据位置最优 spark后期在进行任务调度的时候，会优先考虑存有数据的worker节点来进行任务的计算。大大减少数据的网络传输，提升性能。这里就运用到了<a href="https://cloud.tencent.com/solution/bigdata?from=20065&amp;from_column=20065" target="_blank" rel="noopener noreferrer">大数据</a>中移动数据不如移动计算理念。</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="rdd特点">RDD特点<a href="#rdd特点" class="hash-link" aria-label="Direct link to RDD特点" title="Direct link to RDD特点">​</a></h3>
<p>RDD表示只读的分区的数据集，对RDD进行改动，只能通过RDD的转换操作，由一个RDD得到一个新的RDD，新的RDD包含了从其他RDD衍生所必需的信息。RDDs之间存在依赖，RDD的执行是按照血缘关系延时计算的。如果血缘关系较长，可以通过持久化RDD来切断血缘关系。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="常见算子">常见算子<a href="#常见算子" class="hash-link" aria-label="Direct link to 常见算子" title="Direct link to 常见算子">​</a></h2>
<p>从小方向来说，Spark 算子大致可以分为以下三类:<br>1）Value数据类型的Transformation算子，这种变换并不触发提交作业，针对处理的数据项是Value型的数据。<br>2）Key-Value数据类型的Transfromation算子，这种变换并不触发提交作业，针对处理的数据项是Key-Value型的数据对。<br>3）Action算子，这类算子会触发SparkContext提交Job作业。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="聚合算子">聚合算子<a href="#聚合算子" class="hash-link" aria-label="Direct link to 聚合算子" title="Direct link to 聚合算子">​</a></h3>
<ol>
<li>**ReduceByKeyApache Spark 中用于对键值对 RDD 进行聚合操作的转换算子之一。它对具有相同键的值进行本地归约，然后在整个数据集上进行全局归约。**reduceByKey  的基本作用是将相同键的值按照给定的归约函数进行合并。</li>
</ol>
<p>具体来说，<strong>reduceByKey</strong> 的签名如下：</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">def reduceByKey(func: (V, V) =&gt; V): RDD[(K, V)]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<ol start="2">
<li><strong>groupByKey</strong></li>
</ol>
<p>Apache Spark 中用于对键值对 RDD 进行分组操作的转换算子之一。它将具有相同键的所有值组合在一起，形成一个键对应的值的集合。具体来说，<strong>groupByKey</strong> 的签名如下：</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">def groupByKey(): RDD[(K, Iterable[V])]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<ol start="3">
<li><strong>aggregateByKey</strong></li>
</ol>
<p>Apache Spark 中用于  对键值对 RDD 进行聚合操作的高级转换算子。它提供了更大的灵活性，允许用户在每个键上执行不同的初始化、合并和最终聚合逻辑。<strong>aggregateByKey</strong> 的签名如下：</p>
<div class="language-scala codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-scala codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  def aggregateByKey[U: ClassTag](zeroValue: U, partitioner: Partitioner)(seqOp: (U, V) =&gt; U,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      combOp: (U, U) =&gt; U): RDD[(K, U)] = self.withScope {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    // Serialize the zero value to a byte array so that we can get a new clone of it on each key</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    val zeroBuffer = SparkEnv.get.serializer.newInstance().serialize(zeroValue)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    val zeroArray = new Array[Byte](zeroBuffer.limit)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    zeroBuffer.get(zeroArray)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    lazy val cachedSerializer = SparkEnv.get.serializer.newInstance()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    val createZero = () =&gt; cachedSerializer.deserialize[U](ByteBuffer.wrap(zeroArray))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    // We will clean the combiner closure later in `combineByKey`</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    val cleanedSeqOp = self.context.clean(seqOp)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    combineByKeyWithClassTag[U]((v: V) =&gt; cleanedSeqOp(createZero(), v),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      cleanedSeqOp, combOp, partitioner)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>其中：</p>
<ul>
<li><strong>zeroValue</strong> 是一个初始值，用于为每个键创建一个累加器。</li>
<li><strong>seqOp</strong> 是一个二元操作符，用于将每个值合并到累加器。</li>
<li><strong>combOp</strong> 是一个二元操作符，用于将两个累加器合并成一个。</li>
</ul>
<div class="language-scala codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-scala codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  def main(args: Array[String]): Unit = {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    val sc = SparkSession.builder().appName(&quot;RDD to DataFrame&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      .master(&quot;local[*]&quot;).getOrCreate().sparkContext</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    val testData = Array(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      (&quot;a&quot;, 1), (&quot;a&quot;, 3), (&quot;b&quot;, 4), (&quot;c&quot;, 4), (&quot;b&quot;, 5), (&quot;d&quot;, 3),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      (&quot;a&quot;, 1), (&quot;e&quot;, 3), (&quot;a&quot;, 4), (&quot;f&quot;, 4), (&quot;c&quot;, 5), (&quot;c&quot;, 3),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      (&quot;c&quot;, 1), (&quot;c&quot;, 3), (&quot;b&quot;, 4), (&quot;a&quot;, 4), (&quot;e&quot;, 5), (&quot;e&quot;, 3),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      (&quot;e&quot;, 1), (&quot;f&quot;, 3), (&quot;c&quot;, 4), (&quot;c&quot;, 4), (&quot;c&quot;, 5), (&quot;c&quot;, 3)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    val testRDD = sc.parallelize(testData, 4)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    val resultRDD = testRDD.aggregateByKey(10)(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      (u: Int, v: Int) =&gt; u + v,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      (u1: Int, u2: Int) =&gt; u1 + u2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    resultRDD.foreach(record =&gt; {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      val partIndex = TaskContext.getPartitionId()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      println(&quot;分区：&quot; + partIndex + &quot;,&quot; + record._1 + &quot;=&quot; + record._2)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    })</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<blockquote>
<p>groupByKey、reduceByKey、aggregateByKey区别</p>
</blockquote>
<p>三者都可以做分组操作。reduceByKey、aggregateByKey不但分组还做了聚合操作</p>
<ul>
<li>groupByKey直接进行shuffle操作，数据量大的时候  速度较慢。</li>
<li>reduceByKey、aggregateByKey在shuffle之前可能会先进行聚合，聚合后的数据再进行shuffle，这样一来进行shuffle的数据会变少，速度会快。</li>
<li>reduceByKey、aggregateByKey的区别是前者不同partition内部数据合并以及partition之间的数据的聚合操作是一样的，而后者可以指定两种操作来对应于partition之间和partition内部不同的聚合操作，并且aggregateByKey可以指定初始值。</li>
<li>在aggregateByKey中，如果两种操作是一样的，可以使用foldByKey来代替，并且只传一个操作函数。foldBykey和reudceBykey的区别是前者可以指定一个初始值</li>
</ul>
<ol start="4">
<li><strong>foldByKey</strong></li>
</ol>
<p>是aggregateByKey函数的特殊形式，即简写形式。<br>**功能：**根据设定规则同时进行分区间的计算和分区内的计算，具体为，<br>（1）在分区内按照相同的key进行某种规则计算；<br>（2）分区内部计算完后，接着计算分区间的同样依据相同的key按照规则进行计算，<strong>注意：分区内和分区间的计算规则是一样的。</strong></p>
<p>示例：</p>
<div class="language-scala codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-scala codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">    val sc = SparkSession.builder().appName(&quot;RDD to DataFrame&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      .master(&quot;local[*]&quot;).getOrCreate().sparkContext</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    val testData = Array(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      (&quot;a&quot;, 1), (&quot;a&quot;, 3), (&quot;b&quot;, 4), (&quot;c&quot;, 4), (&quot;b&quot;, 5), (&quot;d&quot;, 3),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      (&quot;a&quot;, 1), (&quot;e&quot;, 3), (&quot;a&quot;, 4), (&quot;f&quot;, 4), (&quot;c&quot;, 5), (&quot;c&quot;, 3),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      (&quot;c&quot;, 1), (&quot;c&quot;, 3), (&quot;b&quot;, 4), (&quot;a&quot;, 4), (&quot;e&quot;, 5), (&quot;e&quot;, 3),</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      (&quot;e&quot;, 1), (&quot;f&quot;, 3), (&quot;c&quot;, 4), (&quot;c&quot;, 4), (&quot;c&quot;, 5), (&quot;c&quot;, 3)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    val testRDD = sc.parallelize(testData, 4)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    val resultRDD = testRDD.foldByKey(10)(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      (u: Int, v: Int) =&gt; u + v</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    resultRDD.foreach(record =&gt; {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      val partIndex = TaskContext.getPartitionId()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      println(&quot;分区：&quot; + partIndex + &quot;,&quot; + record._1 + &quot;=&quot; + record._2)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    })</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<ol start="5">
<li><strong>combineByKey</strong></li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="重新分区算子">重新分区算子<a href="#重新分区算子" class="hash-link" aria-label="Direct link to 重新分区算子" title="Direct link to 重新分区算子">​</a></h3>
<ol>
<li>**repartition	**</li>
</ol>
<p><strong>repartition</strong> 是 Apache Spark 中用于改变 RDD 分区数量的转换算子。它用于重新组织数据并更改 RDD 的分区分布，可以增加或减少分区的数量。<strong>repartition</strong> 的签名如下：</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain"> /**</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   * Return a new RDD that has exactly numPartitions partitions.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   *</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   * Can increase or decrease the level of parallelism in this RDD. Internally, this uses</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   * a shuffle to redistribute data.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   *</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   * If you are decreasing the number of partitions in this RDD, consider using `coalesce`,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   * which can avoid performing a shuffle.</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   */</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  def repartition(numPartitions: Int)(implicit ord: Ordering[T] = null): RDD[T] = withScope {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    coalesce(numPartitions, shuffle = true)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>其中 <strong>numPartitions</strong> 是希望得到的新分区数量。</p>
<ul>
<li>repartition可以将分区的并行度增加，也可以将分区的并行度减少</li>
<li>可以看到repartition调用了coalesce方法，并且传入的shuffle参数是true。换句  说话，就是无论分区数是增加还是减少<strong>都会执行shuffle</strong>操作。</li>
</ul>
<p>使用 <strong>repartition</strong> 时，Spark 将重新分配数据以适应新的分区数，可能需要进行数据的移动和网络传输。因此，谨慎使用 <strong>repartition</strong>，特别是在大规模数据集上，以避免不必要的开销。</p>
<ol start="2">
<li><strong>coalesce</strong></li>
</ol>
<p><strong>coalesce</strong> 是 Spark 中的一个转换算子，用于减少 RDD 的分区数量。与 <strong>repartition</strong> 不同，<strong>coalesce</strong> 主要用于减少分区数量，而不进行数据的 shuffle（不进行数据的网络传输和重分配）。因此，相对于<strong>repartition</strong>，<strong>coalesce</strong> 更加轻量级。<br><strong>coalesce</strong> 的签名如下：</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">def coalesce(numPartitions: Int, shuffle: Boolean = false, partitionCoalescer: Option[PartitionCoalescer] = Option.empty)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (implicit ord: Ordering[T] = null): RDD[T]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>其中：</p>
<ul>
<li><strong>numPartitions</strong> 是希望得到的新分区数量。</li>
<li><strong>shuffle</strong> 是一个布尔值，表示是否进行 shuffle。默认为 <strong>false</strong>，即不进行 shuffle。</li>
<li><strong>partitionCoalescer</strong> 是一个可选参数，用于指定自定义的分区合并器</li>
</ul>
<p>我们常认为coalesce不产生shuffle会比repartition 产生shuffle效率高，而实际情况往往要根据具体问题具体分析，coalesce效率不一定高，有时还有大坑，大家要慎用。 coalesce 与 repartition 他们两个都是RDD的分区进行重新划分，repartition只是coalesce接口中shuffle为true的实现（假设源RDD有N个分区，需要重新划分成M个分区） <br>1）如果 。一般情况下N个分区有数据分布不均匀的状况，利用HashPartitioner函数将数据重新分区为M个，这时需要将shuffle设置为true(repartition实现,coalesce也实现不了)。 <br>2）如果N&gt;M并且N和M相差不多，(假如N是1000，M是100)那么就可以将N个分区中的若干个分区合并成一个新的分区，最终合并为M个分区，这时可以将shuff设置为false（coalesce实现），如果M&gt;N时，coalesce是无效的，不进行shuffle过程，父RDD和子RDD之间是窄依赖关系，无法使文件数(partiton)变多。 总之如果shuffle为false时，如果传入的参数大于现有的分区数目，RDD的分区数不变，也就是说不经过shuffle，是无法将RDD的分区数变多的 <br>3）如果N&gt;M并且两者相差悬殊，这时你要看executor数与要生成的partition关系，如果executor数 小于等于 要生成partition数，coalesce效率高，反之如果用coalesce会导致(executor数-要生成partiton数)个excutor空跑从而降低效率。如果在M为1的时候，为了使coalesce之前的操作有更好的并行度，可以将shuffle设置为true</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="排序算子">排序算子<a href="#排序算子" class="hash-link" aria-label="Direct link to 排序算子" title="Direct link to 排序算子">​</a></h3>
<ol>
<li><strong>sortBy</strong></li>
</ol>
<p><strong>sortBy</strong> 是 Apache Spark 中的一个转换算子，用于对 RDD 中的元素进行排序。与 <strong>sortByKey</strong> 不同，<strong>sortBy</strong> 允许对元素的任意属性进行排序，而不仅仅是键。<strong>sortBy</strong> 的签名如下：</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">def sortBy[K](f: (T) =&gt; K, ascending: Boolean = true, numPartitions: Int = this.partitions.length)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  (implicit ord: Ordering[K], ctag: ClassTag[K]): RDD[T]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>其中：</p>
<ul>
<li><strong>f</strong> 是一个函数，将元素映射到需要排序的键。</li>
<li><strong>ascending</strong> 是一个布尔值，表示升序还是降序排序，默认为升序。</li>
<li><strong>numPartitions</strong> 是结果 RDD 的分区数量，默认为原始 RDD 的分区数量。</li>
<li><strong>ord</strong> 是一个隐式参数，表示排序的顺序，Spark 会根据被排序的类型推断出适当的隐式 <strong>Ordering</strong>。</li>
<li><strong>ctag</strong> 是一个隐式参数，表示排序键的 <strong>ClassTag</strong>。</li>
</ul>
<ol start="2">
<li><strong>sortByKey</strong></li>
</ol>
<p>在一个(K,V)的RDD上调用，K必须实现Ordered接口，返回一个按照key进行排序的(K,V)的RDD</p>
<ul>
<li><strong>ascending</strong>：一个布尔值，表示排序顺序，默认为 <strong>true</strong>，即升序排序。</li>
<li><strong>numPartitions</strong>：结果 RDD 的分区数量，默认为原始 RDD 的分区数量。</li>
<li><strong>ord</strong>：隐式参数，表示排序的顺序，Spark 会根据键的类型推断出适当的隐式 <strong>Ordering</strong>。</li>
<li><strong>ctag</strong>：隐式参数，表示排序键的 <strong>ClassTag</strong></li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="映射算子">映射算子<a href="#映射算子" class="hash-link" aria-label="Direct link to 映射算子" title="Direct link to 映射算子">​</a></h3>
<ol>
<li><strong>map</strong></li>
</ol>
<p><strong>map</strong> 是 Apache Spark 中的一个转换算子，用于对 RDD 中的每个元素执行指定的映射操作。它将一个 RDD 中的每个元素通过一个用户定义的函数进行映射，生成一个新的 RDD。<strong>map</strong> 的基本签名如下：</p>
<div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">def map[U: ClassTag](f: T =&gt; U): RDD[U]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<ol start="2">
<li><strong>flatMap</strong></li>
</ol>
<p>flatMap 其实和 map 与 mapPartitions 算子类似，在功能上，与 map 和 mapPartitions 一样，flatMap 也是用来做数据映射的，在实现上，对于给定映射函数 f，flatMap(f) 以元素为粒度，对 RDD 进行数据转换。不过，与前两者相比，flatMap 的映射函数 f 有着显著的不同。对于 map 和 mapPartitions 来说，其映射函数 f 的类型，都是（元素） =&gt; （元素），即元素到元素。而 flatMap 映射函数 f 的类型，是（元素） =&gt; （集合），即元素到集合（如数组、列表等）。因此，flatMap 的映射过程在逻辑上分为两步：</p>
<ul>
<li>以元素为单位，创建集合；</li>
<li>去掉集合“外包装”，提取集合元素。</li>
</ul>
<div class="language-scala codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-scala codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">// 读取文件内容</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val lineRDD: RDD[String] = sc.parallelize(List(&quot;Apache Spark is a fast and general-purpose cluster computing system.&quot;))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">// 以行为单位提取相邻单词</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val wordPairRDD: RDD[String] = lineRDD.flatMap( line =&gt; {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  // 将行转换为单词数组</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  val words: Array[String] = line.split(&quot; &quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  // 将单个单词数组，转换为相邻单词数组</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  val list = for (i &lt;- 0 until words.length - 1) yield words(i) + &quot;-&quot; + words(i+1)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  println(&quot;list: &quot; + list)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  list.iterator  //返回list的迭代器,最终所有的集合元素都会被抽到一个集合中</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">})</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">wordPairRDD.collect().foreach(x=&gt;println(x))</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<ol start="3">
<li><strong>mapPartition</strong>mapPartitions** 是 Apache Spark 中的一个转换算子，它允许对 RDD 中的每个分区应用一个函数。与 <strong>map</strong> 和 <strong>flatMap</strong> 不同，<strong>mapPartitions</strong> 操作的是整个分区的数据而不是单个元素，这使得它更适合处理分区级别的操作，而不是每个元素的操作。</li>
<li><strong>mapPartitionWithIndex</strong></li>
</ol>
<p><strong>mapPartitionsWithIndex</strong> 是 Apache Spark 中的一个转换算子，类似于 <strong>mapPartitions</strong>，但它额外提供了分区的索引信息。这个算子允许对 RDD 中的每个分区应用一个函数，并传递分区索引作为参数。这样，你可以在处理每个分区时根据分区索引执行不同的操作。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="去重算子">去重算子<a href="#去重算子" class="hash-link" aria-label="Direct link to 去重算子" title="Direct link to 去重算子">​</a></h3>
<ol>
<li><strong>distinct</strong></li>
</ol>
<p><strong>distinct</strong> 是 Apache Spark 中的一个转换算子，用于去除 RDD 中的重复元素，返回一个包含唯一元素的新的 RDD<br><strong>distinit原理</strong><br>我们看下源码</p>
<div class="language-scala codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-scala codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">/**   * Return a new RDD containing the distinct elements in this RDD.   */  </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">def distinct(numPartitions: Int(implicit ord: Ordering[T] = null): RDD[T] = withScope {    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    map(x =&gt; (x, null)).reduceByKey((x, y) =&gt; x, numPartitions).map(_._1)  </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}  </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">/**   * Return a new RDD containing the distinct elements in this RDD.   */</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">def distinct(): RDD[T] = withScope {    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    distinct(partitions.length)  </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>上面是distinct的源码，有带参和无参两种。当我们调用无参的distinct时，底层调用的是如下源码</p>
<div class="language-python codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-python codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">def</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">distinct</span><span class="token punctuation" style="color:#393A34">(</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">:</span><span class="token plain"> RDD</span><span class="token punctuation" style="color:#393A34">[</span><span class="token plain">T</span><span class="token punctuation" style="color:#393A34">]</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> withScope </span><span class="token punctuation" style="color:#393A34">{</span><span class="token plain">    </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  distinct</span><span class="token punctuation" style="color:#393A34">(</span><span class="token plain">partitions</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">length</span><span class="token punctuation" style="color:#393A34">)</span><span class="token plain">  </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token punctuation" style="color:#393A34">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p>带参数的distinct其内部就很容易理解了，这就是一个wordcount统计单词的方法，区别是：后者通过元组获取了第一个单词元素。<br><img decoding="async" loading="lazy" src="https://cdn.nlark.com/yuque/0/2023/webp/390265/1702985724430-8753831e-58aa-4e4c-a756-249422c57e14.webp#averageHue=%23f6f6f6&amp;clientId=u4d1050a1-fe82-4&amp;from=paste&amp;id=uf0a688b6&amp;originHeight=263&amp;originWidth=640&amp;originalType=url&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;taskId=ucaf51d76-56d4-4b1e-9607-f12911cb409&amp;title=" alt="" class="img_ev3q"><br>总结：使用map算子把元素转为一个带有null的元组；使用reducebykey对具有相同key的元素进行统计；之后再使用map算子，取得元组中的单词元素，实现去重的效果。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="shuffer算法">Shuffer算法<a href="#shuffer算法" class="hash-link" aria-label="Direct link to Shuffer算法" title="Direct link to Shuffer算法">​</a></h2>
<blockquote>
<p>Spark有2种Shuffer方法，第一种是HashShuffer，第二种是SortShuffer。</p>
</blockquote>
<p>HashShuffer在旧的版本中先使用的，类似于MapReduce,但是区别在于Spark中的HashShuffer没有排序，而MapReduce HashShuffer中是有排序的，但是后面的版本中，Spark将HashShuffer换成了SortShuffer。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="hashshuffer的迭代">HashShuffer的迭代<a href="#hashshuffer的迭代" class="hash-link" aria-label="Direct link to HashShuffer的迭代" title="Direct link to HashShuffer的迭代">​</a></h3>
<p><strong>普通hashShuffer</strong><br>第一版是直接Map端的Task直接根据下游的Reduce的Task的数量然后将对应属于reducetask分区的数据写入到本地磁盘文件。 <br>假设Map端有2个Executor，每个Executor中有2个Map task，而下游有一个Reduce，Reduce中有3个task。如下图：<br><img decoding="async" loading="lazy" alt="img.png" src="/assets/images/img-f11ca5d0830f2dd0a7ca601ee0aaf95c.png" width="750" height="455" class="img_ev3q"><br>Map端的task将内存中的数据分别写入到本地的多个文件中，每个文件对应的是下游中的一个reduce task。由于总共有3个reduce task，所有一个map task 需要生成三个reduce task的文件。<br>这样的话，2个executor中有4个task，每个map task 需要生成3个reduce task。就需要生成4*3 = 12 个task文件。<br>task文件数量= MapTask数量 * ReduceTask的数量。 <br>这样会有一个问题，当MapTask和ReduceTask的数量比较多时，会导致task文件的数量爆炸性的增长。IO性能会降低。所以后面Spark抛弃了这种方式。<br>参考：</p>
<ul>
<li><a href="https://alstonwilliams.github.io/spark/2019/02/17/Spark%E6%9E%B6%E6%9E%84-Shuffle(%E8%AF%91)/" target="_blank" rel="noopener noreferrer">https://alstonwilliams.github.io/spark/2019/02/17/Spark%E6%9E%B6%E6%9E%84-Shuffle(%E8%AF%91)/</a></li>
</ul>
<p><strong>V2ConsolidatedShuffle（文件合并的HashShuffer）</strong><br>基于第一版HashShuffer的缺点(生成的task文件太多，会导致内存需要存储大量的文件描述符，而且会引起海量的IO读写)，Spark又使用了第二版的Shuffer算法。<br>为了优化 HashShuffleManager 我们可以设置一个参数：spark.shuffle.consolidateFiles，该参数默认值为 false，将其设置为 true 即可开启优化机制，通常来说，如果我们使用 HashShuffleManager，那么都建议开启这个选项。</p>
<p>开启 consolidate 机制之后，在 shuffle write 过程中，task 就不是为下游 stage 的每个 task 创建一个磁盘文件了，此时会出现shuffleFileGroup的概念，每个 shuffleFileGroup 会对应一批磁盘文件，磁盘文件的数量与下游 stage 的 task 数量是相同的。一个 Executor 上有多少个 cpu core，就可以并行执行多少个 task。而第一批并行执行的每个 task 都会创建一个 shuffleFileGroup，并将数据写入对应的磁盘文件内。</p>
<p>当 Executor 的 cpu core 执行完一批 task，接着执行下一批 task 时，下一批 task 就会复用之前已有的 shuffleFileGroup，包括其中的磁盘文件，也就是说，此时 task 会将数据 写入已有的磁盘文件中，而不会写入新的磁盘文件中。因此，consolidate 机制允许不同的 task 复用同一批磁盘文件，这样就可以有效将多个 task 的磁盘文件进行一定程度上的合并，从而大幅度减少磁盘文件的数量，进而提升 shuffle write 的性能。</p>
<p>假设第二个 stage 有 100 个 task，第一个 stage 有 50 个 task，总共还是有 10 个 Executor（Executor CPU 个数为 1），每个 Executor 执行 5 个 task。那么原本使用未经优化的 HashShuffleManager 时，每个 Executor 会产生 500 个磁盘文件，所有 Executor 会产生 5000 个磁盘文件的。但是此时经过优化之后，每个 Executor 创建的磁盘文件的数量的计算公式为：cpu core的数量 * 下一个stage的task数量，也就是说，每个 Executor 此时只会创建 100 个磁盘文件，所有 Executor 只会创建 1000 个磁盘文件。<br>这个功能优点明显，但为什么 Spark 一直没有在基于 Hash Shuffle 的实现中将功能设置为默认选项呢，官方给出的说法是这个功能还欠稳定。<br>Consolidation算法就是多了一个文件的合并， 从之前的每个MapTask生成文件优化为每个Execotor为单位生成文件(其实应该是以Core核心数为单位，这里默认了一个Executor只有一个Core核心)，这个合并其实是指单分区的合并，最终以一个文件为一个分区。<br>ConsolidatedShuffle 以Executor为单位，进行文件处理，如下图：<br><img decoding="async" loading="lazy" alt="img_1.png" src="/assets/images/img_1-a6448261ec695375882126e15ff75235.png" width="750" height="457" class="img_ev3q"><br>和第一版不同的是，以Executor为单位生成对应的Task文件，这样的话Task文件的数量就会大幅的减少，为Executor的数量*ReeuceTask的数量。但是如果ReduceTask的数量依然还是比较多的话， 产生的task文件数还是很多。 最终Spark转向了SortShuffer.<br>总结：</p>
<ol>
<li>HashShuffer的优点是不用排序，因为每个下游Task的数据都被单独写到一个文件中， 所以可以不用排序。</li>
<li>缺点是生产的文件过多，会对文件系统造成压力。大量小文件随机读写带来一定的磁盘开销。</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="sort-based-shuffer">Sort Based Shuffer<a href="#sort-based-shuffer" class="hash-link" aria-label="Direct link to Sort Based Shuffer" title="Direct link to Sort Based Shuffer">​</a></h3>
<p>参考：<a href="https://blog.csdn.net/weixin_39216383/article/details/81194498" target="_blank" rel="noopener noreferrer">https://blog.csdn.net/weixin_39216383/article/details/81194498</a><br>由于HashShuffer会导致产生大量的中间磁盘文件，导致IO性能降低。尽管后面有合并文件的HashShuffer，但是当ReduceTask数量大的时候依然会I产生大量的文件导致IO性能降低。<br>因此Spark在1.2版本正式使用了Sort Shuffer替代了Hash Shuffer。因此目前我们在Spark中使用到的Shuffer算法都是Sort Base Shuffer。<br>在Sort Shuffer中，分两类的场景，一类是普通的SortShuffer，一类是开启byPass的Sort Shuffer。什么情况下使用普通SortShuffer，什么情况下使用ByPass的Sort取决于是有没有聚合运算。<br><strong>普通SortShuffer</strong><br>会对partition中数据在内存中进行排序。然后分批次写入到磁盘的临时文件中，最后把多个临时文件合并为一个文件(多分区数据合并)。这种模式和Mapreduce差不多，数据会先写入一个内存数据结构中，如果是ReduceByKey这种带有聚合的算子，那么内存结构会使用Map结构。如果是Join这种shuffer算子， 则会使用ArrayList的数据结构， 数据直接写入内存，当内存数据量达到临界值时，会将内存数据写入到磁盘中。在写入磁盘前，会按key对每个partition中的数据进行排序，排序后按批次写入磁盘文件， 最后再进行合并到一个文件。因此产生的  文件数量和Shuffer Write Task。的数量相同。如下图：<br><img decoding="async" loading="lazy" src="https://cdn.nlark.com/yuque/0/2023/jpeg/390265/1685340542432-2e9baa86-ea14-4b0b-b798-6f1793dbb31c.jpeg#averageHue=%23f0f085&amp;clientId=u63baf342-ef31-4&amp;from=paste&amp;id=u726c43e6&amp;originHeight=1084&amp;originWidth=1308&amp;originalType=url&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;taskId=ufbd26bc4-2f5a-410f-b9fe-d5e2537c17c&amp;title=" alt="" class="img_ev3q"><br><strong>byPass机制的SortShuffer(忽略sort)</strong><br><img decoding="async" loading="lazy" src="https://cdn.nlark.com/yuque/0/2023/jpeg/390265/1685340586141-27f7a003-301e-48bb-a4bd-442c16a3a16f.jpeg#averageHue=%23f2f276&amp;clientId=u63baf342-ef31-4&amp;from=paste&amp;id=u91d5e5f0&amp;originHeight=1102&amp;originWidth=1714&amp;originalType=url&amp;ratio=1&amp;rotation=0&amp;showTitle=false&amp;status=done&amp;style=none&amp;taskId=u16243c38-a0b0-4f09-b9d9-a0bdeace85a&amp;title=" alt="" class="img_ev3q"><br>byPass的启动机制：</p>
<ul>
<li>shuffle reduce task数量小于spark.shuffle.sort.bypassMergeThreshold参数的值，这个值默认200个。</li>
<li>不是聚合类的shuffle算子（比如reduceByKey）。</li>
</ul>
<p>byPass的特殊之处在于不会对partition内的数据进行排序。、Reducer 端任务数比较少的情况下，基于 Hash Shuffle 实现机制明显比基于 Sort Shuffle 实现机制要快，因此基于 Sort huffle 实现机制提供了一个回退方案，就是 bypass 运行机制。对于 Reducer 端任务数少于配置属性spark.shuffle.sort.bypassMergeThreshold设置的个数时，使用带 Hash 风格的回退计划。<br>此时，每个 task 会为每个下游 task 都创建一个临时磁盘文件，并将数据按 key 进行 hash 然后根据 key 的 hash 值，将 key 写入对应的磁盘文件之中。当然，写入磁盘文件时也是先写入内存缓冲，缓冲写满之后再溢写到磁盘文件的。最后，同样会将所 有临时磁盘文件都合并成一个磁盘文件，并创建一个单独的索引文件。<br>该过程的磁盘写机制其实跟未经优化的 HashShuffleManager 是一模一样的，因为都要创建数量惊人的磁盘文件，只是在最后会做一个磁盘文件的合并而已。因此少量的最终磁盘文件，也让该机制相对未经优化的 HashShuffleManager 来说，shuffle read 的性能会更好。<br>而该机制与普通 SortShuffleManager 运行机制的不同在于：第一，磁盘写机制不同；第二，不会进行排序。也就是说，启用该机制的最大好处在于，shuffle write 过程中，不需要进行数据的排序操作，也就节省掉了这部分的性能开销。<br><strong>总结:</strong></p>
<ol>
<li>如果有聚合条件的Shuffer算子(比如ReduceByKey)，使用普通机制的SortShuffer，会进行排序</li>
<li>如果不是聚合条件的Shuffer算子，且Shuffer Read Task的数量小于等于spark.shuffle.sort.bypassMergeThreshold的值(默认为200)，就会启用byPass机制，byPass不会进行排序。可以减少排序带来的性能损耗。</li>
<li>两种SortShuffer产生的task文件数量和Map Task 的数量相同。 与ReduceTask数量没有关系。比如2个Executor，每个Executor中有2个MapTask。那么就会产生4个task文件 ，这个文件是将下游分区的数据Merge一起了。并带有一个索引文件，Reduce端拉取的时候会先通过索引文件确定要读取的文件的数据范围。</li>
<li>为什么HashShuffer中没有进行sort,是为了去掉sort这个阶段，如果没有了sort，那么就意味着最终生成的多个文件不能合并，MapReduce中shuffer是用归并排序，前提是每个文件中已经做了排序。但是spark没有sort，所以最终的文件合并成本很高，基本就不可能合并。所以Map Task 对下游的每个Reduce单独生成了一个partition了。能够合并的前提是按一个文件中要按partition排序，这样reduce正确拉取到。</li>
<li>  在第二版的HashShuffer中，对Map Task 生成的文件进行了合并，合并规则是按照partition进行合并，最终在一个Executor中的文件个数与Reduce的数量是一致的。一个分区一个文件。</li>
</ol>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="任务提交流程">任务提交流程<a href="#任务提交流程" class="hash-link" aria-label="Direct link to 任务提交流程" title="Direct link to 任务提交流程">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="spark组件角色">Spark组件角色<a href="#spark组件角色" class="hash-link" aria-label="Direct link to Spark组件角色" title="Direct link to Spark组件角色">​</a></h3>
<ul>
<li>Application：用户编写的Spark应用程序，包含了一个Driver 功能的代码和分布在集群中多个节点上运行的Executor代码</li>
<li>Driver：运行Application的main()函数并且创建SparkContext（Spark应用程序的运行环境）。Driver负责和ClusterManager通信，进行资源的申请、任务的分配和监控等。</li>
<li>Cluster Manager：集群上获取资源的外部服务，比如Standalone（由Master负责资源的分配）和Yarn（由ResourceManager负责资源的分配）</li>
<li>Worker：从节点，负责控制计算节点，启动Executor或者Driver。在Standalone模式中指的就是通过Slave文件配置的Worker节点，在Spark on Yarn模式中指的就是NodeManager节点。</li>
<li>Executor：运行在Worker 节点上的进程，该进程负责运行Task，并且负责将数据存在内存或者磁盘上。</li>
<li>Job：包含多个Task组成的并行计算，往往由Spark Action催生，一个JOB包含多个RDD及作用于相应RDD上的各种Operation；</li>
<li>Stage：每个Job会被拆分很多Stage，而每个Stage又包含多个Task；Stage是根据宽依赖和窄依赖划分的.</li>
<li>Task：被送到某个Executor上的工作任务</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="提交方式">提交方式<a href="#提  交方式" class="hash-link" aria-label="Direct link to 提交方式" title="Direct link to 提交方式">​</a></h3>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="standalone-client方式提交任务方式">Standalone-Client方式提交任务方式<a href="#standalone-client方式提交任务方式" class="hash-link" aria-label="Direct link to Standalone-Client方式提交任务方式" title="Direct link to Standalone-Client方式提交任务方式">​</a></h4>
<div class="language-shel codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shel codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">spark-submit  --master spark://node001:7077,node002:7077 --deploy-mode client --class org.apache.spark.examples.SparkPi ../examples/jars/spark examples_2.11-2.3.1.jar  10000</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><img decoding="async" loading="lazy" alt="img_11.png" src="/assets/images/img_11-79ba6bbb96c36f65308b38dd87940287.png" width="637" height="444" class="img_ev3q">
standalone-client模式</p>
<p>执行流程</p>
<ol>
<li>Client模式提交任务后，会在客户端启动Driver进程</li>
<li>Driver会向Master申请启动Application启动的资源</li>
<li>资源申请成功，Driver端将task发送到worker端执行</li>
<li>worker将task执行结果返回到Driver端。</li>
</ol>
<p><strong>缺点</strong></p>
<p>client模式适用于测试调试程序。Driver进程是在客户端启动的，这里的客户端就是指提交应用程序的当前节点。在Driver端可以看到task执行的情况。生产环境下不能使用client模式，因为Driver可能会回收task执行结果数据，假设要提交100个application到集群运行，Driver每次都会在client端启动，那么就会导致客户端所在节点的Driver收集100个application的结果数据，导致100次网卡流量暴增的问题。</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="standalone-cluster方式提交任务方式"><strong>Standalone-Cluster方式提交任务方式</strong><a href="#standalone-cluster方式提交任务方式" class="hash-link" aria-label="Direct link to standalone-cluster方式提交任务方式" title="Direct link to standalone-cluster方式提交任务方式">​</a></h4>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">spark-submit  --master spark://node001:7077,node002:7077 --deploy-mode cluster --class org.apache.spark.examples.SparkPi ../examples/jars/spark examples_2.11-2.3.1.jar  10000</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><img decoding="async" loading="lazy" alt="img_12.png" src="/assets/images/img_12-5726d3ef146dd38af6a4d05ed5088611.png" width="640" height="394" class="img_ev3q"></p>
<p>执行流程</p>
<ul>
<li>1.cluster模式提交应用程序后，会向Master请求启动Driver.</li>
<li>2.Master接受请求，随机在集群一台节点启动Driver进程。</li>
<li>3.Driver启动后为当前的应用程序申请资源。</li>
<li>4.Driver端发送task到worker节点上执行。</li>
<li>5.worker将执行情况和执行结果返回给Driver端。</li>
</ul>
<p><strong>总结</strong></p>
<p>Driver进程是在集群某一台Worker上启动的，在客户端是无法查看task的执行情况的。假设要提交100个application到集群运行,每次Driver会随机在集群中某一台Worker上启动，那么这100次网卡流量暴增的问题就散布在集群上。</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="yarn-client方式提交任务方式"><strong>Yarn-Client方式提交任务方式</strong><a href="#yarn-client方式提交任务方式" class="hash-link" aria-label="Direct link to yarn-client方式提交任务方式" title="Direct link to yarn-client方式提交任务方式">​</a></h4>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">spark-submit  --master yarn --deploy-mode client --class org.apache.spark.examples.SparkPi ../examples/jars/spark examples_2.11-2.3.1.jar  10000</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><img decoding="async" loading="lazy" alt="img_13.png" src="/assets/images/img_13-3145e481f94c80de029a2b03a4421bec.png" width="640" height="429" class="img_ev3q"></p>
<p>执行流程</p>
<ul>
<li>1.客户端提交一个Application，在客户端启动一个Driver进程。</li>
<li>2.应用程序启动后会向RS(ResourceManager)发送请求，启动AM(ApplicationMaster)的资源。</li>
<li>3.RS收到请求，随机选择一台NM(NodeManager)启动AM。这里的NM相当于Standalone中的Worker节点。</li>
<li>4.AM启动后，会向RS请求一批container资源，用于启动Executor。</li>
<li>5.RS会找到一批NM返回给AM,用于启动Executor。</li>
</ul>
<p><strong>总结</strong></p>
<p>Yarn-Client模式同样是适用于测试，因为Driver运行在本地，Driver会与yarn集群中的Executor进行大量的通信，会造成客户机网卡流量的大量增加。</p>
<p><strong>Yarn-Client模式下ApplicationMaster的作用：</strong></p>
<ul>
<li>1.为当前的Application申请资源</li>
<li>2.给NameNode发送消息启动Executor。 注意：ApplicationMaster有launchExecutor和申请资源的功能，并没有作业调度的功能。因此进程名称为ExecutorLauncher，不能叫做ApplicationMaster。</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="yarn-cluster方式提交任务方式"><strong>Yarn-Cluster方式提交任务方式</strong><a href="#yarn-cluster方式提交任务方式" class="hash-link" aria-label="Direct link to yarn-cluster方式提交任务方式" title="Direct link to yarn-cluster方式提交任务方式">​</a></h4>
<div class="language-shell codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-shell codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">spark-submit  --master yarn --deploy-mode cluster --class org.apache.spark.examples.SparkPi ../examples/jars/spark examples_2.11-2.3.1.jar  10000</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<p><img decoding="async" loading="lazy" alt="img_14.png" src="/assets/images/img_14-9cf0c9abefcf129fba51a83916417fa9.png" width="640" height="391" class="img_ev3q"></p>
<p>执行流程</p>
<ul>
<li>1.客户机提交Application应用程序，发送请求到RS(ResourceManager),请求启动AM(ApplicationMaster)。</li>
<li>2.RS收到请求后随机在一台NM(NodeManager)上启动AM（相当于Driver端）。</li>
<li>3.AM启动，AM发送请求到RS，请求一批container用于启动Executor。</li>
<li>4.RS返回一批NM节点给AM。</li>
<li>5.AM连接到NM,发送请求到NM启动Executor。</li>
<li>6.Executor反向注册到AM所在的节点的Driver。Driver发送task到Executor。</li>
</ul>
<p><strong>总结</strong></p>
<p>Yarn-Cluster主要用于生产环境中，因为Driver运行在Yarn集群中某 一台nodeManager中，每次提交任务的Driver所在的机器都是随机的，不会产生某一台机器网卡流量激增的现象，缺点是任务提交后不能看到日志。只能通过yarn查看日志。</p>
<p><strong>Yarn-Cluster模式下ApplicationMaster的作用：</strong></p>
<ul>
<li>1.为当前的Application申请资源</li>
<li>2.给NameNode发送消息启动Executor。</li>
<li>3.任务调度。 注意：ApplicationMaster有launchExecutor和申请资源的功能，相比较Yarn-Client模式下具备了作业调度的功能。因此进程名称叫做ApplicationMaster。</li>
</ul>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="数据倾斜">数据倾斜<a href="#数据倾斜" class="hash-link" aria-label="Direct link to 数据倾斜" title="Direct link to 数据倾斜">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="什么是数据倾斜">什么是数据倾斜<a href="#什么是数据倾斜" class="hash-link" aria-label="Direct link to 什么是数据倾斜" title="Direct link to 什么是数据倾斜">​</a></h3>
<p>数据清洗是某个partitin的数据量非常大， 而别的分区的数据量小。导致几个task计算耗时很久任务计算时间很长。就是某个key在shuffer的时候被分发到一个分区，而这个key的数量远大于其他的key.</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="数据倾斜会导致什么问题">数据倾斜会导致什么问题<a href="#数据倾斜会导致什么问题" class="hash-link" aria-label="Direct link to 数据倾斜会导致什么问题" title="Direct link to 数据倾斜会导致什么问题">​</a></h3>
<ol>
<li>导致OOM</li>
<li>导致任务计算时间很长</li>
</ol>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="如何解决数据倾斜">如何解决数据倾斜<a href="#如何解决数据倾斜" class="hash-link" aria-label="Direct link to 如何解决数据倾斜" title="Direct link to 如何解决数据倾斜">​</a></h3>
<ul>
<li>参考： <a href="https://zhuanlan.zhihu.com/p/511193855" target="_blank" rel="noopener noreferrer">https://zhuanlan.zhihu.com/p/511193855</a></li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="基本思路">基本思路<a href="#基本思路" class="hash-link" aria-label="Direct link to 基本思路" title="Direct link to 基本思路">​</a></h4>
<p>既然数据倾斜是因为相同 key 的值被分配到少数几个节点上造成的单点问题，那么尽可能的的让 key 平均分配，问题就解决了。所以可以有几个思路：</p>
<ul>
<li>数据预处理：在数据清洗过程中把计数特别多的 key 过滤掉（有损），或单独计算（无损）。</li>
<li>业务逻辑优化：从业务逻辑的层面上来优化数据倾斜，比如从数据的源头就尽可能避免单个计量很大的 key</li>
<li>参数优化：Hadoop 和 Spark 都自带了很多的参数和机制来调节数据倾斜，合理利用它们就能解决大部分问题。</li>
<li>程序优化：比如用 group 代替 count(distinct) 等。</li>
</ul>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="过滤异常数据">过滤异常数据<a href="#过滤异常数据" class="hash-link" aria-label="Direct link to 过滤异常数据" title="Direct link to 过滤异常数据">​</a></h4>
<p>直接过滤异常数据，对于有的情况下是有损的，但是对于部分情况，该方法也是可用的，比如本该在数据清洗阶段清洗的大量的NULL值、空值未被清洗。对于这类情况，直接使用 where 条件过滤即可。<br>另外，如果部分看起来有用的数据，但是预估到实际计算结果中影响不大，也可酌情过滤。</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="优化元数据">优化元数据<a href="#优化元数据" class="hash-link" aria-label="Direct link to 优化元数据" title="Direct link to 优化元数据">​</a></h4>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="优化map端数据">优化Map端数据<a href="#优化map端数据" class="hash-link" aria-label="Direct link to  优化Map端数据" title="Direct link to 优化Map端数据">​</a></h4>
<p>如果Map端从HDFS读取的数据为不压缩文件，且不支持split切割，则会导致有的Map端读取的文件比较大，有的文件读取的文件较小。产生Map端的数据倾斜。</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="优化shuffle逻辑">优化Shuffle逻辑<a href="#优化shuffle逻辑" class="hash-link" aria-label="Direct link to 优化Shuffle逻辑" title="Direct link to 优化Shuffle逻辑">​</a></h4>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="使用mapjoin">使用MapJoin<a href="#使用mapjoin" class="hash-link" aria-label="Direct link to 使用MapJoin" title="Direct link to 使用MapJoin">​</a></h4>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="调整reduce并行度">调整reduce并行度<a href="#调整reduce并行度" class="hash-link" aria-label="Direct link to 调整reduce并行度" title="Direct link to 调整reduce并行度">​</a></h4>
<p>调整reduce的并行度，从而使得每个partition分区分发到的数据量会减少。从而解决数据倾斜。</p>
<h4 class="anchor anchorWithStickyNavbar_LWe7" id="key-加盐">key 加盐<a href="#key-加盐" class="hash-link" aria-label="Direct link to key 加盐" title="Direct link to key 加盐">​</a></h4>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="checkpoint">Checkpoint<a href="#checkpoint" class="hash-link" aria-label="Direct link to Checkpoint" title="Direct link to Checkpoint">​</a></h2>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="缓存">缓存<a href="#缓存" class="hash-link" aria-label="Direct link to 缓存" title="Direct link to 缓存">​</a></h2>
<p>RDD 可以使用 persist() 方法或 cache() 方法进行持久化。数据将会在第一次 action 操作时进行计算，并缓存在节点的内存中。Spark 的缓存具有容错机制，如果一个缓存的 RDD 的某个分区丢失了，Spark 将按照原来的计算过程，自动重新计算并进行缓存。<br>持久化是将RDD  的数据缓存到内存或者其他存储介质中(比如磁盘)。<br>RDD本身是不存储数据的，如果要想一个RDD实现复用，则会重新从源头开始计算数据。导致数据被重新计算。<br>如果一个RDD缓存的数据丢失了，则会重新计算数据。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="cache">Cache<a href="#cache" class="hash-link" aria-label="Direct link to Cache" title="Direct link to Cache">​</a></h3>
<p>Cache和Persist方法其实表达的是同一个意思，只不过Cache方法也调用的是persist方法。<br>Cache对某个RDD的数据进行缓冲到内存中，实现了数据的复用。下次重复使用该RDD的时候不会从数据源进行重新的计算，提高了计算效率和减少了重复的计算量。<br>源代码如下，其实cache还是调用了persist()的无参方法，默认存储到内存中。</p>
<div class="language-scala codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-scala codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">def persist(): this.type = persist(StorageLevel.MEMORY_ONLY)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">/**</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">* Persist this RDD with the default storage level (`MEMORY_ONLY`).</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">*/</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">def cache(): this.type = persist()</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<div class="language-scala codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-scala codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">package com.dytg.sparkcore</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">import org.apache.spark.{SparkConf, SparkContext}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">object Cache01 {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  def main(args: Array[String]): Unit = {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    val sparkConf = new SparkConf().setAppName(&quot;Example021&quot;).setMaster(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    val sc = new SparkContext(sparkConf)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    val rdd = sc.makeRDD(List(&quot;zhangsan&quot;, &quot;lisi&quot;, &quot;wangwu&quot;));</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    val mapRdd = rdd.map(e =&gt;{</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      println(&quot;执行map 算子&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      e+&quot; : 20 &quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    } )</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    mapRdd.cache()</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    mapRdd.foreach(e =&gt; println(e))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    //如果对maprdd不进行cache缓存，那么执行saveAsTextFile时会顺着RDD血缘重新再计算一次。</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    //虽然这里mapRDD这个对象被复用了，但是RDD中本身是不存储数据的，只是对数据的抽象的描述和定义。</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    //所以需要追溯到数据源头再进行一次计算。</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    //当mapRdd执行了cache，则会将数据缓存到内存中，再一次使用mapRDD的时候，就直接使用cache的数据，不用重新开始计算了，这是一种减少</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    //重复计算量的一种方法</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    mapRdd.saveAsTextFile(&quot;rdd.txt&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="persist">Persist<a href="#persist" class="hash-link" aria-label="Direct link to Persist" title="Direct link to Persist">​</a></h3>
<p>persist方法的作用和cache方法一样，只不过persist方法支持配置数据存储的方式，可以存储到内存 中，也可以存储到文件中，但是这里的存储到文件中是一个临时文件，当Job停止的时候会自动删除这个临时的文件。因此只在一个Job运行的过程中可以使用到。</p>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="存储级别">存储级别<a href="#存储级别" class="hash-link" aria-label="Direct link to 存储级别" title="Direct link to 存储级别">​</a></h3>
<ul>
<li>MEMORY_ONLY : 将 RDD 以反序列化 Java 对象的形式存储在 JVM 中。如果内存空间不够，部分数据分区将不再缓存，在每次需要用到这些数据时重新进行计算。这是默认的级别。</li>
<li>MEMORY_AND_DISK : 将 RDD 以反序列化 Java 对象的形式存储在 JVM 中。如果内存空间不够，将未缓存的数据分区存储到磁盘，在需要使用这些分区时从磁盘读取。</li>
<li>MEMORY_ONLY_SER : 将 RDD 以序列化的 Java 对象的形式进行存储（每个分区为一个 byte 数组）。这种方式会比反序列化对象的方式节省很多空间，尤其是在使用 fast serializer时会节省更多的空间，但是在读取时会增加 CPU 的计算负担。</li>
<li>MEMORY_AND_DISK_SER : 类似于 MEMORY_ONLY_SER ，但是溢出的分区会存储到磁盘，而不是在用到它们时重新计算。</li>
<li>DISK_ONLY : 只在磁盘上缓存 RDD。</li>
<li>MEMORY_ONLY_2，MEMORY_AND_DISK_2，等等 : 与上面的级别功能相同，只不过每个分区在集群中两个节点上建立副本。</li>
<li>OFF_HEAP（实验中）: 类似于 MEMORY_ONLY_SER ，但是将数据存储在 off-heap memory，这需要启动 off-heap 内存。</li>
</ul>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="缓存删除策略">缓存删除策略<a href="#缓存删除策略" class="hash-link" aria-label="Direct link to 缓存删除策略" title="Direct link to 缓存删除策略">​</a></h3>
<p>Spark 自动监控各个节点上的缓存使用率，并以最近最少使用的方式（LRU）将旧数据块移除内存。如果  想手动移除一个 RDD，而不是等待该 RDD 被 Spark 自动移除，可以使用 RDD.unpersist() 方法。</p>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="累加器和广播变量">累加器和广播变量<a href="#累加器和广播变量" class="hash-link" aria-label="Direct link to 累加器和广播变量" title="Direct link to 累加器和广播变量">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="累加器">累加器<a href="#累加器" class="hash-link" aria-label="Direct link to 累加器" title="Direct link to 累加器">​</a></h3>
<p>在spark应用程序中，我们经常会有这样的需求，如异常监控，调试，记录符合某特性的数据的数目，这种需求都需要用到计数器，如果一个变量不被声明为一个累加器，那么它将在被改变时不会再driver端进行全局汇总，即在分布式运行时每个task运行的只是原始变量的一个副本，并不能改变原始变量的值，但是当这个变量被声明为累加器后，该变量就会有分布式计数的功能。<br>错误的图解：<br><img decoding="async" loading="lazy" alt="img_2.png" src="/assets/images/img_2-047b69f4eedae791fe236d4818ad5675.png" width="750" height="287" class="img_ev3q"><br>正确的图解<br><img decoding="async" loading="lazy" alt="img_3.png" src="/assets/images/img_3-fba703bcc6c9fe3c3b9658df6917734a.png" width="750" height="261" class="img_ev3q"><br>注意事项：<br>累加器在Driver端定义赋初始值，累加器只能在Driver端读取最后的值，在Excutor端更新<br>累加器的定义</p>
<div class="language-java codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-java codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">val a = sc.accumulator(0) //定 义一个累加器</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">val b = a.value //获取一个累加器的值</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="广播变量">广播变量<a href="#广播变量" class="hash-link" aria-label="Direct link to 广播变量" title="Direct link to 广播变量">​</a></h3>
<p>如果我们要在分布式计算里面分发大对象，例如：字典，集合，黑白名单等，这个都会由Driver端进行分发，一般来讲，如果这个变量不是广播变量，那么每个task就会分发一份，这在task数目十分多的情况下Driver的带宽会成为系统的瓶颈，而且会大量消耗task服务器上的资源，如果将这个变量声明为广播变量，那么知识每个executor拥有一份，这个executor启动的task会共享这个变量，节省了通信的成本和服务器的资源。<br><strong>广播变量的特点</strong></p>
<ol>
<li>由Driver端分发到每个Executor上</li>
<li>每个Executor上的task共享读取该广播变量</li>
<li>广播变量不能被修改</li>
<li>一般用于字典表，或者join小表的广播</li>
</ol>
<p>广播变量图解<br>错误用法，会导致每个task都有一份变量的数据，导致浪费Executor的内存<br><img decoding="async" loading="lazy" alt="img_4.png" src="/assets/images/img_4-a98529537c85ee962e4aeb92ea424909.png" width="750" height="333" class="img_ev3q"><br>正确用法。使用广播变量分发到每个Executor上，Executor上的task共享该变量，变量内存只有一份<br><img decoding="async" loading="lazy" alt="img_5.png" src="/assets/images/img_5-8417767ac41180fa50c1aa035a3c4161.png" width="750" height="337" class="img_ev3q"><br>下面是一个代码示例：<br>要通过broast.value来获取广播变量的值</p>
<div class="language-java codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-java codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">val sparkConf = new SparkConf().setAppName(&quot;Example0212&quot;).setMaster(&quot;local[*]&quot;)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    val sc = new SparkContext(sparkConf)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    val rdd = sc.makeRDD(List((&quot;a&quot;, 4), (&quot;b&quot;, 1), (&quot;c&quot;, 2)))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    val map = mutable.Map((&quot;a&quot;,1),(&quot;b&quot;,2),(&quot;c&quot;,4))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    val broastMap: Broadcast[mutable.Map[String, Int]] = sc.broadcast(map)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    rdd.map{</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    case (word,count) =&gt;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        val l =  broastMap.value.getOrElse(word,0)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        (word,(count,l))</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">        }.foreach(println)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div>
<h2 class="anchor anchorWithStickyNavbar_LWe7" id="dataframe和dataset">DataFrame和DataSet<a href="#dataframe和dataset" class="hash-link" aria-label="Direct link to DataFrame和DataSet" title="Direct link to DataFrame和DataSet">​</a></h2>
<h3 class="anchor anchorWithStickyNavbar_LWe7" id="区别">区别<a href="#区别" class="hash-link" aria-label="Direct link to 区别" title="Direct link to 区别">​</a></h3>
<p>DataFrame和DataSet都是分布式数据处理的抽象概念，用于表示分布式数据集。它们之间的主要区别在于其对类型安全性和编程接口的支持。</p>
<ol>
<li><strong>DataFrame:</strong>
<ul>
<li><strong>概念：</strong> DataFrame 是一种分布式的数据集，可以看作是由命名列组成的分布式表格。它类似于关系型数据库中的表格或者Pandas库中的DataFrame。</li>
<li><strong>类型安全性：</strong> DataFrame 是不类型安全的，即它的列没有特定的类型信息，而是被认为是通用的<strong>Column</strong>类型。这意味着在编译时无法捕捉到类型错误。</li>
<li><strong>编程接口：</strong> DataFrame API 是基于SQL的查询语言，提供了一种类似于SQL的查询接口，支持使用SQL语句进行数据操作。DataFrame可以通过Spark SQL模块创建，也可以通过读取外部数据源创建。</li>
</ul>
</li>
<li><strong>DataSet:</strong>
<ul>
<li><strong>概念：</strong> DataSet 是强类型的分布式数据集，它是对DataFrame的扩展，引入了类型信息。它可以看作是具有类型信息的分布式集合，类似于Scala或Java中的集合。</li>
<li><strong>类型安全性：</strong> DataSet 是类型安全的，即它在编译时就能捕捉到类型错误。这使得在进行操作时更容易发现潜在的问题。</li>
<li><strong>编程接口：</strong> DataSet API 提供了一组强类型的操作函数，可以直接使用类和Lambda表达式。它更适用于编写复杂的业务逻辑，而不仅仅是简单的数据操作。DataSet也可以通过Spark SQL模块创建。</li>
</ul>
</li>
</ol>
<p>其实DataFrame可以看做是特殊的DataSet，数据类型为Row，看源码</p>
<div class="language-scala codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-scala codeBlock_bY9V thin-scrollbar" style="color:#393A34;background-color:#f6f8fa"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">  type DataFrame = Dataset[Row]</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="Copy code to clipboard" title="Copy" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div></div></article><nav class="pagination-nav docusaurus-mt-lg" aria-label="Docs pages"><a class="pagination-nav__link pagination-nav__link--prev" href="/docs/Spark/Spark面试题2"><div class="pagination-nav__sublabel">Previous</div><div class="pagination-nav__label">Spark面试题2</div></a><a class="pagination-nav__link pagination-nav__link--next" href="/docs/Spark/SparkStreamingCheckpoint机制"><div class="pagination-nav__sublabel">Next</div><div class="pagination-nav__label">SparkStreamingCheckpoint机制</div></a></nav></div></div><div class="col col--3"><div class="tableOfContents_bqdL thin-scrollbar theme-doc-toc-desktop"><ul class="table-of-contents table-of-contents__left-border"><li><a href="#什么是rdd2" class="table-of-contents__link toc-highlight">什么是RDD2</a><ul><li><a href="#概念" class="table-of-contents__link toc-highlight">概念</a></li><li><a href="#rdd的五个特性" class="table-of-contents__link toc-highlight">RDD的五个特性</a></li><li><a href="#rdd特点" class="table-of-contents__link toc-highlight">RDD特点</a></li></ul></li><li><a href="#常见算子" class="table-of-contents__link toc-highlight">常见算子</a><ul><li><a href="#聚合算子" class="table-of-contents__link toc-highlight">聚合算子</a></li><li><a href="#重新分区算子" class="table-of-contents__link toc-highlight">重新分区算子</a></li><li><a href="#排序算子" class="table-of-contents__link toc-highlight">排序算子</a></li><li><a href="#映射算子" class="table-of-contents__link toc-highlight">映射算子</a></li><li><a href="#去重算子" class="table-of-contents__link toc-highlight">去重算子</a></li></ul></li><li><a href="#shuffer算法" class="table-of-contents__link toc-highlight">Shuffer算法</a><ul><li><a href="#hashshuffer的迭代" class="table-of-contents__link toc-highlight">HashShuffer的迭代</a></li><li><a href="#sort-based-shuffer" class="table-of-contents__link toc-highlight">Sort Based Shuffer</a></li></ul></li><li><a href="#任务提交流程" class="table-of-contents__link toc-highlight">任务提交流程</a><ul><li><a href="#spark组件角色" class="table-of-contents__link toc-highlight">Spark组件角色</a></li><li><a href="#提交方式" class="table-of-contents__link toc-highlight">提交方式</a></li></ul></li><li><a href="#数据倾斜" class="table-of-contents__link toc-highlight">数据倾斜</a><ul><li><a href="#什么是数据倾斜" class="table-of-contents__link toc-highlight">什么是数据倾斜</a></li><li><a href="#数据倾斜会导致什么问题" class="table-of-contents__link toc-highlight">数据倾斜会导致什么问题</a></li><li><a href="#如何解决数据倾斜" class="table-of-contents__link toc-highlight">如何解决数据倾斜</a></li></ul></li><li><a href="#checkpoint" class="table-of-contents__link toc-highlight">Checkpoint</a></li><li><a href="#缓存" class="table-of-contents__link toc-highlight">缓存</a><ul><li><a href="#cache" class="table-of-contents__link toc-highlight">Cache</a></li><li><a href="#persist" class="table-of-contents__link toc-highlight">Persist</a></li><li><a href="#存储级别" class="table-of-contents__link toc-highlight">存储级别</a></li><li><a href="#缓存删除策略" class="table-of-contents__link toc-highlight">缓存删除策略</a></li></ul></li><li><a href="#累加器和广播变量" class="table-of-contents__link toc-highlight">累加器和广播变量</a><ul><li><a href="#累加器" class="table-of-contents__link toc-highlight">累加器</a></li><li><a href="#广播变量" class="table-of-contents__link toc-highlight">广播变量</a></li></ul></li><li><a href="#dataframe和dataset" class="table-of-contents__link toc-highlight">DataFrame和DataSet</a><ul><li><a href="#区别" class="table-of-contents__link toc-highlight">区别</a></li></ul></li></ul></div></div></div></div></main></div></div></div></div>
</body>
</html>